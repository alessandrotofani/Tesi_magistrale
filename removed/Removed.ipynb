{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Removed.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO/O8MLkgcY+WCd70zrm1KQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandrotofani/Tesi_magistrale/blob/master/Removed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmsmwq2zkpMR"
      },
      "source": [
        "# Removed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPA8z7HQkub_"
      },
      "source": [
        "Rimpiazzo i NaN con il valore 0 (da rivedere con le tecniche per rimpiazzare i missing values. \r\n",
        "\r\n",
        "Faccio il one hot encoding per avere solamente feature numeriche. (anche questo può essere riconsiderato)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8su9puEgks0n"
      },
      "source": [
        "# elimino i nan andandoli a rimpiazzare con zero \r\n",
        "data = data.replace(to_replace = np.nan, value = 0)\r\n",
        "\r\n",
        "# faccio il one hot encoding delle feature categoriche così da avere solo valori numerici\r\n",
        "data = pd.get_dummies(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya4fxL8gkyFV"
      },
      "source": [
        "sec_in_day = 86400\r\n",
        "# tempo da cui inizio a selezionare\r\n",
        "start = sec_in_day - 1\r\n",
        "# tempo in cui si conclude lo split \r\n",
        "end = sec_in_day * 2 - 1\r\n",
        "# creo un array che contiene i dati suddivisi per giorno\r\n",
        "sub_data = []\r\n",
        "# for i in range(int(total_days)):\r\n",
        "for i in range(10):\r\n",
        "#   per concatenare due condizioni devo delimitarle con le parentesi \r\n",
        "#   https://stackoverflow.com/questions/31617845/how-to-select-rows-in-a-dataframe-between-two-values-in-python-pandas\r\n",
        "    sub_data.append(data[(data['TransactionDT'] < end) & (data['TransactionDT'] > start)]);\r\n",
        "    start += sec_in_day\r\n",
        "    end += sec_in_day\r\n",
        "# sub_data\r\n",
        "\r\n",
        "fraud = []\r\n",
        "safe = []\r\n",
        "fraud_distr = []\r\n",
        "for i in range(9):\r\n",
        "    fraud_distr.append(sub_data[i][['TransactionDT','TransactionAmt','isFraud']]) \r\n",
        "    fraud.append( fraud_distr[i][fraud_distr[i]['isFraud'] == 1])\r\n",
        "    safe.append(fraud_distr[i][fraud_distr[i]['isFraud'] == 0])\r\n",
        "\r\n",
        "x = []\r\n",
        "y = []\r\n",
        "# https://matplotlib.org/3.3.2/gallery/subplots_axes_and_figures/subplot.html#sphx-glr-gallery-subplots-axes-and-figures-subplot-py\r\n",
        "# https://stackoverflow.com/questions/17210646/python-subplot-within-a-loop-first-panel-appears-in-wrong-position\r\n",
        "fig, ax = plt.subplots(3, 3, figsize = (30,10))\r\n",
        "fig.subplots_adjust(wspace = 1)\r\n",
        "ax = ax.ravel()\r\n",
        "# for loop per plottare i vari subplots\r\n",
        "for i in range(9):\r\n",
        "    x.append([fraud[i]['TransactionDT'], safe[i]['TransactionDT']])\r\n",
        "    y.append([fraud[i]['TransactionAmt'], safe[i]['TransactionAmt']])\r\n",
        "\r\n",
        "    fig.suptitle('Amount nel tempo')\r\n",
        "\r\n",
        "    ax[i].plot(x[i][0], y[i][0], label = 'fraud')\r\n",
        "    ax[i].plot( x[i][1],  y[i][1], alpha = 0.5, label = 'safe')\r\n",
        "#     ax[i].set_title('Day')\r\n",
        "#     ax[i].set_ylabel('amount')\r\n",
        "    ax[i].set_xlabel('time (s)')\r\n",
        "\r\n",
        "plt.show()\r\n",
        "\r\n",
        "dev_info = original_data[['DeviceInfo', 'isFraud']]\r\n",
        "dev_info = dev_info[dev_info['DeviceInfo'] != 0]\r\n",
        "dev_info_fraud = dev_info[dev_info['isFraud'] == 1].value_counts(normalize = True)\r\n",
        "dev_info_safe = dev_info[dev_info['isFraud'] == 0].value_counts(normalize = True)\r\n",
        "\r\n",
        "dev_info_fraud[:10].plot(kind = 'bar', label = 'fraud', color = 'red');\r\n",
        "dev_info_safe[:10].plot(kind = 'bar',  label='safe', color = 'green');\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_LoPJMEk0zm"
      },
      "source": [
        "#### Altro modo (peggiore) per fare l'iterative imputer su tutto il dataset\r\n",
        "Faccio l'iterative imputer su tutto il dataset però tenendo solo la colonna transactionAmt -> più veloce ma non è comunque corretto dal punto di vista concettuale. L'algoritmo stima i valori mancanti attraverso tutti gli altri valori contenuti nel dataset. Se gli dessi solo una colonna, potrebbe non essere sufficiente per valutare bene i missing values. \r\n",
        "\r\n",
        "Creo un dizionario (fitted_set) che conterrà le colonne con i valori stimati. \r\n",
        "\r\n",
        "Faccio un loop su tutte le colonne per stimare i valori. \r\n",
        "\r\n",
        "Con round setto il numero di decimali. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.round.html\r\n",
        "\r\n",
        "fitted è il dataframe che contiene i valori rimpiazzati, quindi senza NaN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2yb0asYk3Lv"
      },
      "source": [
        "do = False\r\n",
        "\r\n",
        "if do:\r\n",
        "    # dizionario che contiene le colonne con i valori rimpiazzati\r\n",
        "    fitted_set = {}\r\n",
        "\r\n",
        "    for col in num_data.columns:\r\n",
        "        if col != 'TransactionAmt':\r\n",
        "            subset = num_data[['TransactionAmt',col]]\r\n",
        "            imp = IterativeImputer(missing_values=np.nan, random_state=0, n_nearest_features=5)                          \r\n",
        "            imp.fit(subset)\r\n",
        "            subset = imp.transform(subset)\r\n",
        "            fitted_set[col] = subset[:,1]\r\n",
        "\r\n",
        "    fitted = pd.DataFrame(fitted_set).round(2)\r\n",
        "    del fitted_set\r\n",
        "    # fitted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI5iRFQCk7lL"
      },
      "source": [
        "## Correlazione senza rimpiazzo dei missing values \r\n",
        "Cerco correlazioni tra le features. \r\n",
        "Vado ad eliminare le colonne con troppi zeri che sono meno informative. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL8leKgek8E3"
      },
      "source": [
        "do = False \r\n",
        "if do: \r\n",
        "    data_splitted = split_by_day(original_data, 1)\r\n",
        "\r\n",
        "    # dizionario che contengono la matrice con i feature vector\r\n",
        "    X = {}\r\n",
        "    # dizionario che contiene le label\r\n",
        "    y = {}\r\n",
        "\r\n",
        "    # popolo X e y\r\n",
        "    for i in range(n_set):\r\n",
        "        X[i] = data_splitted[i].drop(['isFraud'], axis = 1)\r\n",
        "        y[i] = data_splitted[i]['isFraud']\r\n",
        "\r\n",
        "    # dizionario con i vari dataset per la correlazione\r\n",
        "    X_correlation = {}\r\n",
        "    thresh = 2000\r\n",
        "    sub_dataset = 0\r\n",
        "    # https://stackoverflow.com/questions/46628253/deleting-dataframe-column-in-pandas-based-on-value\r\n",
        "    # https://stackoverflow.com/questions/53550988/count-occurrences-of-false-or-true-in-a-column-in-pandas\r\n",
        "    X_correlation[sub_dataset] = X[sub_dataset].loc[:, ((X[sub_dataset] != 0).sum() > thresh)]\r\n",
        "    # X_correlation[sub_dataset]\r\n",
        "    \r\n",
        "    # https://stackoverflow.com/questions/29432629/plot-correlation-matrix-using-pandas\r\n",
        "    f = plt.figure(figsize=(40, 40))\r\n",
        "    plt.matshow(X_correlation[sub_dataset].corr(), fignum=f.number)\r\n",
        "    plt.xticks(range(X_correlation[sub_dataset].shape[1]), X_correlation[sub_dataset].columns, fontsize=10, rotation=90)\r\n",
        "    plt.yticks(range(X_correlation[sub_dataset].shape[1]), X_correlation[sub_dataset].columns, fontsize=10)\r\n",
        "    cb = plt.colorbar()\r\n",
        "    cb.ax.tick_params(labelsize=14)\r\n",
        "    plt.title('Correlation Matrix', fontsize=16);"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJhmO91gk-J-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}