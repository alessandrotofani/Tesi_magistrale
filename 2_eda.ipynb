{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_eda.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPuWRLanOIN2EK03f4d4Kci",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandrotofani/Tesi_magistrale/blob/master/2_eda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tUxyYqcIV88"
      },
      "source": [
        "import numpy as np # linear algebra\r\n",
        "import pandas as pd # data processing\r\n",
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import os\r\n",
        "import mf\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "for dirname, _, filenames in os.walk('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE'):\r\n",
        "    for filename in filenames:\r\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzweXCGBIjz7"
      },
      "source": [
        "original_data = pd.read_csv('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/original_data.csv')\r\n",
        "original_data.drop(original_data.columns[original_data.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdJLDhyWIMyV"
      },
      "source": [
        "numerical_data = pd.read_csv('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/fitted.csv')\r\n",
        "numerical_data.drop(numerical_data.columns[numerical_data.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InVD_76xIbEA"
      },
      "source": [
        "## 1.2 Dataset split in numerical e categorical\r\n",
        "\r\n",
        "Seleziono le feature i cui dati sono numerici. \r\n",
        "\r\n",
        "https://pandas.pydata.org/pandas--docs/stable/reference/api/pandas.DataFrame.select_dtypes.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll_QFDBsIYGv"
      },
      "source": [
        "num_data = numerical_data.drop(['isFraud','TransactionID','TransactionDT'], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGJqNc8jIdby"
      },
      "source": [
        "categorical_col = []\r\n",
        "categorical_col.append('TransactionID')\r\n",
        "categorical_col.append('isFraud')\r\n",
        "for col in original_data:\r\n",
        "  if col not in numerical_cols:\r\n",
        "    categorical_col.append(col)\r\n",
        "\r\n",
        "len(categorical_col)\r\n",
        "categorical_data = original_data[original_data.columns.intersection(categorical_col)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLT6YmacIqar"
      },
      "source": [
        "# 2. Exploratory analysis\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goMifM8KIslv"
      },
      "source": [
        "Vedo su quanti giorni va il dataset e controllo la proporzione tra eventi fraudolenti e non. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF2g5loMIfR2"
      },
      "source": [
        "total_days = np.ceil(max(original_data['TransactionDT'])/(86400))\r\n",
        "print('Dataset spans ', total_days, ' days')\r\n",
        "\r\n",
        "fraud = (original_data['isFraud'] == 1).sum()\r\n",
        "safe = (original_data['isFraud'] == 0).sum()\r\n",
        "print('Fraudolent events: ', fraud)\r\n",
        "print('Safe events: ', safe)\r\n",
        "print('Ratio Fraud/safe: ', fraud/(fraud + safe))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZrGRCHSX0iQ"
      },
      "source": [
        "## 2.1. Analisi feature numeriche\n",
        "\n",
        "Analizzo la distribuzione e la significatività statistica delle feature numeriche. \n",
        "\n",
        "Boxplot per confrontare la distribuzione delle feature numeriche tra transazioni fraudolente e non. \n",
        "\n",
        "Riferimento: https://stackoverflow.com/questions/62166292/seaborn-catplot-is-throwing-error-truth-value-is-ambiguous\n",
        "\n",
        "Riferimento map_dataframe: https://stackoverflow.com/questions/35131798/tweaking-seaborn-boxplot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gTSHgzyKX0iR"
      },
      "source": [
        "melted = []\n",
        "n_plot = 3 # len(cat_cols)\n",
        "target_col = \"isFraud\"\n",
        "cat_cols = num_data.columns[(num_data.dtypes == int) | (num_data.dtypes == float)]\n",
        "for i in range(n_plot):\n",
        "    melted.append(numerical_data.melt(id_vars=target_col,value_vars=cat_cols[i]))\n",
        "    g = sns.FacetGrid(melted[i], col='variable', sharex=False,col_wrap=1)\n",
        "    g.map_dataframe(sns.boxplot, x=\"isFraud\", y=\"value\", showfliers=False, hue = 'isFraud')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuKwvv9RX0iR"
      },
      "source": [
        "Visualizzo la differenza nella distribuzione di probabilità della feature a seconda della label. \r\n",
        "\r\n",
        "Documentazione histplot: https://seaborn.pydata.org/generated/seaborn.histplot.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "sCyymWUeX0iS"
      },
      "source": [
        "sns.histplot(data=numerical_data[['card1','isFraud']], x=\"card1\", hue=\"isFraud\", element=\"step\", stat=\"density\", common_norm=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H08ltjIxX0iS"
      },
      "source": [
        "Calcolo le medie e le deviazioni standard per poter calcolare la variabile t ed effettuare il test di Welch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "khIj6LE6X0iS"
      },
      "source": [
        "means = mf.get_stat(numerical_data, mean=True)\n",
        "stds = mf.get_stat(numerical_data, std=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JmfbDDB8X0iT"
      },
      "source": [
        "safe_numerical = mf.get_subFrame(numerical_data, safe = True)\n",
        "fraud_numerical = mf.get_subFrame(numerical_data, fraud = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt-rW_UiX0iT"
      },
      "source": [
        "Definisco le funzioni per poter effettuare il Welch test e lo effettuo. \r\n",
        "\r\n",
        "$ t = \\frac{\\mu_{0} - \\mu_{1}}{\\sqrt{\\frac{s_{0}^{2}}{N_{0}} + \\frac{s_{1}^{2}}{N_{1}}}}$\r\n",
        "\r\n",
        "$ \\nu = \\frac{(\\frac{s_{0}^{2}}{N_{0}} + \\frac{s_{1}^{2}}{N_{1}})^{2}}{{\\frac{s_{0}^{4}}{N_{0}^{2}\\nu_{0}} + \\frac{s_{1}^{4}}{N_{1}^{2}\\nu_{1}}}}$\r\n",
        " \r\n",
        "\r\n",
        "`t` e `dof` sono due dizionari. `t` contiene i valori della variabile t, e `dof` i gradi di libertà. \r\n",
        "\r\n",
        "Riferimento wikipedia Welch t-test:  https://en.wikipedia.org/wiki/Welch%27s_t-test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2ppsws1qX0iT"
      },
      "source": [
        "t_variable = mf.t(means, stds, safe_numerical, fraud_numerical)\n",
        "dof = mf.v(safe_numerical, fraud_numerical)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g2pdy0CX0iU"
      },
      "source": [
        "Una volta effettuato il test, scelgo il livello di significatività e vado a selezionare le feature con p-value superiore a tale livello. \n",
        "\n",
        "Calcolo del p-value: https://towardsdatascience.com/inferential-statistics-series-t-test-using-numpy-2718f8f9bf2f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_99iR6aCX0iU"
      },
      "source": [
        "`num_sign_col`: lista con le colonne numeriche significative tramite test di Welch e con missing values rimpiazzati tramite MICE. \n",
        "\n",
        "Riferimento scrittura file testo: https://stackoverflow.com/questions/899103/writing-a-list-to-a-file-with-python\n",
        "\n",
        "Riferimento lettura file testo: https://www.kite.com/python/answers/how-to-read-a-text-file-into-a-list-in-python\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fgZTloZfX0iU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56f0b41c-6028-4a9d-dfe1-335d2e3c0052"
      },
      "source": [
        "if not os.path.isfile('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/num_sign_col_mice.txt'):\n",
        "    num_sign_col = sig_cols(t_variable, safe_numerical)\n",
        "    with open('num_sign_col_mice.txt', 'w') as f:\n",
        "        for item in num_sign_col:\n",
        "            f.write(\"%s \" % item)\n",
        "            \n",
        "file = open(\"/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/num_sign_col_mice.txt\", \"r\")\n",
        "num_sign_col = file.read()\n",
        "num_sign_col = num_sign_col.split(\" \")\n",
        "file.close()\n",
        "num_sign_col.pop()\n",
        "len(num_sign_col)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RalFheDTX0iU"
      },
      "source": [
        "## 2.2. Analisi feature categoriche\n",
        "Analizzo il comportamento delle feature categoriche. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzOTjLjLX0iU"
      },
      "source": [
        "Istogrammi delle feature categoriche. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TPbb_RSWX0iV"
      },
      "source": [
        "done = False\n",
        "if done:\n",
        "  for col in categorical_col[3:10]:\n",
        "      ax = sns.catplot(data = categorical_data, x=col, hue='isFraud', kind = 'count')\n",
        "      ax.set(yscale=\"log\")\n",
        "      plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9_21ZQ6X0iV"
      },
      "source": [
        "Inizializzo i dizionari che conterranno i conteggi e le frequenze relative a ciascuna feature. Inoltre elimino le colonne isFraud e TransactionID. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2Uwv4xzzX0iV"
      },
      "source": [
        "count = {}\n",
        "frequencies = {}\n",
        "done = False\n",
        "if not done:\n",
        "    categorical_col.remove('isFraud')\n",
        "    categorical_col.remove('TransactionID')\n",
        "    done = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BofFJye0X0iV"
      },
      "source": [
        "Riferimento `group_by`: https://stackoverflow.com/questions/42563209/how-to-count-subgroups-of-categorical-data-in-a-pandas-dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mffLKunyX0iV"
      },
      "source": [
        "for col in categorical_col:\n",
        "    count[col] = categorical_data.groupby('isFraud')[col].value_counts().unstack(fill_value=0)\n",
        "    frequencies[col] = categorical_data.groupby('isFraud')[col].value_counts(normalize = True).unstack(fill_value=0)    \n",
        "#     print(frequencies[col], '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3EIw3JkX0iW"
      },
      "source": [
        "## Test $\\chi^2$\n",
        "Importo la funzione `chi2_contingency` che permette di effettuare il test $\\chi^2$. \n",
        "\n",
        "$H_{0}$: le differenze tra le frequenze nel caso di transazioni fraudolente e non, non siano statisticamente significative, cioè che siano il frutto di noise nel dataset. \n",
        "\n",
        "Risultato: Se il p-value è inferiore al livello di significatività del test, allora la differenza tra le frequenze è significativa, cioè non è casuale. \n",
        "\n",
        "Riferimenti `chi2_contingency`: \n",
        "\n",
        "* https://www.geeksforgeeks.org/python-pearsons-chi-square-test/\n",
        "\n",
        "* https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whoWed2UX0iW"
      },
      "source": [
        "`cat_sign_col`: lista che contiene il nome delle colonne, con variabili categorihe, significative tramite test $\\chi^{2}$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "egi6GxAEX0iW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cdde460-d294-4678-d05f-b6da62a2887a"
      },
      "source": [
        "if not os.path.isfile('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/cat_sign_col.txt'):\n",
        "    cat_sign_col = mf.get_sign_cols(count)\n",
        "    with open('cat_sign_col.txt', 'w') as f:\n",
        "        for item in cat_sign_col:\n",
        "            f.write(\"%s \" % item)\n",
        "            \n",
        "file = open(\"/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/cat_sign_col.txt\", \"r\")\n",
        "cat_sign_col = file.read() # importo il file\n",
        "cat_sign_col = cat_sign_col.split(\" \") # le colonne sono separate dallo spazio\n",
        "file.close() \n",
        "cat_sign_col.pop() # levo l'ultimo elemento che è vuoto"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uGYzjycyK_L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eeb77f7-ff9a-4266-f436-21f2ad0fc3e1"
      },
      "source": [
        "print(len(cat_sign_col), ' significative features on', len(categorical_col), ' total features')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30  significative features on 31  total features\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpvugRX9X0iW"
      },
      "source": [
        "## 2.3 Analisi delle correlazioni\n",
        "\n",
        "#### Funzioni:\n",
        "\n",
        "* `dropColNotSign(dataset, col_sign, not_ignore = None)`: funzione per eliminare le colonne non significative dal dataset. `not_ignore`: colonna da ritenere significativa anche se non presente nella lista `col_sign`. \n",
        "\n",
        "* `corr_matrix_plot(dataset, corr_matrix)`: funzione per plottare la matrice di correlazione. \n",
        "\n",
        "* `highest_correlations(corr_matrix, tresh = 0.8)`: restituisce le features con correlazione superiore alla soglia `tresh` specificata.\n",
        "\n",
        "* `corr_dict(corr)`: restituisce un dizionario i cui elementi sono le coppie di features correlate, con relativo valore di correlazione. \n",
        "\n",
        "#### Variabili: \n",
        "\n",
        "`col_sign` è una lista che conterrà le colonne significative, cioè quelle selezionate per portare avanti l'analisi. \n",
        "\n",
        "`corr_d`: dizionario con le coppie di features correlate e relativa correlazione. \n",
        "\n",
        "#### Riferimenti \n",
        "\n",
        "* Riferimento sul sort: https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\n",
        "\n",
        "- Riferimento `drop_duplicates`: https://stackoverflow.com/questions/30530663/how-to-select-distinct-across-multiple-data-frame-columns-in-pandas\n",
        "\n",
        "* Riferimento `corr_sorted[corr_sorted>tresh]`: https://stackoverflow.com/questions/32067054/remove-rows-of-zeros-from-a-pandas-series\n",
        "\n",
        "* Riferimento `corr_dict`: https://stackoverflow.com/questions/25929319/how-to-iterate-over-pandas-multiindex-dataframe-using-index\n",
        "\n",
        "* Riferimento `np.isnan`: https://numpy.org/doc/stable/reference/generated/numpy.isnan.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UMbqDkl6X0iX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "533277d9-618d-46a5-9e10-91f8ea10adbf"
      },
      "source": [
        "col_sign = num_sign_col + cat_sign_col # lista con le features significative\n",
        "print('Number of significative features: ', len(col_sign))\n",
        "correlation_data = mf.dropColNotSign(numerical_data, num_sign_col) # dataset solo con le features significative\n",
        "correlation_data_fraud = mf.dropColNotSign(numerical_data, num_sign_col, not_ignore = 'isFraud') \n",
        "correlation_data_fraud = correlation_data_fraud[correlation_data_fraud['isFraud'] == 1 ]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of significative features:  148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rtjyiKqcX0iX"
      },
      "source": [
        "show_plots = False\n",
        "if show_plots:\n",
        "  corr_matrix = correlation_data.corr()\n",
        "  mf.corr_matrix_plot(correlation_data, corr_matrix)\n",
        "  highest_corr = mf.highest_correlations(corr_matrix)\n",
        "  highest_corr= highest_corr.unstack(level = 1)\n",
        "\n",
        "  corr_matrix_fraud = correlation_data_fraud.corr()\n",
        "  mf.corr_matrix_plot(correlation_data_fraud, corr_matrix_fraud)\n",
        "  highest_corr_fraud = mf.highest_correlations(corr_matrix_fraud)\n",
        "  highest_corr_fraud = highest_corr_fraud.unstack(level = 1)\n",
        "\n",
        "  corr_d, corr_list = mf.corr_dict(highest_corr)\n",
        "  corr_d_fraud, corr_list_fraud = mf.corr_dict(highest_corr_fraud)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHAdvgpJrAIJ"
      },
      "source": [
        "Vado ad individuare le correlazioni che ci sono solo nel caso di transazioni fraudolente. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "91djjeN5X0iX"
      },
      "source": [
        "if show_plots:\n",
        "  unique_corr = []\n",
        "  for item in corr_list_fraud:\n",
        "      if item not in corr_list:\n",
        "        unique_corr.append(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAqXbMxNX0iY"
      },
      "source": [
        "Fare differenza delle correlazioni nel caso fraud e safe per vedere se spuntano correlazioni diverse. \r\n",
        "\r\n",
        "Riferimento `scatterplot`: https://seaborn.pydata.org/generated/seaborn.scatterplot.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1-eM6qwvX0iY"
      },
      "source": [
        "done = False\r\n",
        "if done:\r\n",
        "  for item in unique_corr:\r\n",
        "    sns.scatterplot(data=correlation_data[item], x=item[0], y=item[1])\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}