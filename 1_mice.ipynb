{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_mice.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMlCnj1yRmksuveNlU/gFxP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandrotofani/Tesi_magistrale/blob/master/1_mice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "HHkvMkxQFZVN",
        "outputId": "cf7ba461-9d32-4c9d-f7ac-38aa970a0b75"
      },
      "source": [
        "import numpy as np # linear algebra\r\n",
        "import pandas as pd # data processing\r\n",
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import os\r\n",
        "import mf\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "for dirname, _, filenames in os.walk('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE'):\r\n",
        "    for filename in filenames:\r\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bab0dd07f987>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mf'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBfDcrG8F5II"
      },
      "source": [
        "# 1. Import dataset e descrizione feature\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIu_0637F7F0"
      },
      "source": [
        "* TransactionDT: timedelta from a given reference datetime\r\n",
        "\r\n",
        "* TransactionAMT: transaction payment amount in USD\r\n",
        "\r\n",
        "* card4: mastercard, visa and other.\r\n",
        "\r\n",
        "* card6: credit or debit.\r\n",
        "\r\n",
        "* P_ and (R__) emaildomain: purchaser and recipient email domain. Categorical: gmail.com, hotmail.com and others.\r\n",
        "\r\n",
        "ProductCD: product code, the product for each transaction. Categorical. \r\n",
        "\r\n",
        "C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\r\n",
        "\r\n",
        "D1-D15: timedelta, such as days between previous transaction, etc.\r\n",
        "\r\n",
        "M1-M9: match, such as names on card and address, etc. Categorical: T or F.  \r\n",
        "\r\n",
        "Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\r\n",
        "\r\n",
        "* DeviceType: type of device used, mobile or desktop.\r\n",
        "\r\n",
        "* DeviceInfo: info on device.\r\n",
        "\r\n",
        "* id_30: OS name and version.\r\n",
        "\r\n",
        "* id_31: browser name and version.\r\n",
        "\r\n",
        "* id_33: screen size.\r\n",
        "\r\n",
        "id_35 to 38: T or F.\r\n",
        "\r\n",
        "id_34: match_status:1 or 2.\r\n",
        " \r\n",
        "id_15 and 28: New or Found. Maybe Found if a customer is registered in Vesta database. \r\n",
        "\r\n",
        "id_12 and 16: Found or NotFound. \r\n",
        "\r\n",
        "id_27: Null of Found.\r\n",
        "\r\n",
        "Vado ad importare il training set\r\n",
        "\r\n",
        "Unisco i due dataset tramite la funzione `merge`, che li unisce rispetto alla colonna `TransactionID`. \r\n",
        "\r\n",
        "`original_data` conterrà il dataset originale, senza rimpiazzamento dei missing values nè encoding delle features categoriche. \r\n",
        "\r\n",
        "Funzione `merge`: https://stackoverflow.com/questions/41463119/join-two-dataframes-on-common-column-in-python\r\n",
        "\r\n",
        "`to_csv`: https://datatofish.com/export-dataframe-to-csv/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_2svlUkF6qk"
      },
      "source": [
        "if not os.path.isfile('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/original_data.csv'):\r\n",
        "    data_identity = pd.read_csv('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/train_identity.csv')\r\n",
        "    data_transaction = pd.read_csv('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/train_transaction.csv')\r\n",
        "    # unisco i due dataframe\r\n",
        "    data = pd.merge(data_transaction, data_identity, left_on='TransactionID', right_on='TransactionID', how='left')\r\n",
        "    original_data = data\r\n",
        "\r\n",
        "    del data\r\n",
        "    del data_transaction\r\n",
        "    del data_identity\r\n",
        "\r\n",
        "    original_data.to_csv(r'./original_data.csv')\r\n",
        "\r\n",
        "original_data = pd.read_csv('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/original_data.csv')\r\n",
        "original_data.drop(original_data.columns[original_data.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7XPBJLEGEqL"
      },
      "source": [
        "## Missing values: MICE method\r\n",
        "\r\n",
        "Seleziono i NaN dal dataframe e li conto. \r\n",
        "\r\n",
        "Costruisco `nans`, che è un dataframe che contiene (feature, nan_totali). \r\n",
        "\r\n",
        "https://stackoverflow.com/questions/28503445/assigning-column-names-to-a-pandas-series\r\n",
        "\r\n",
        "Plotto i NaN contati per ogni feature. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahTBve52F-qo"
      },
      "source": [
        "nan_values = original_data.isna().sum()\r\n",
        "nans = pd.DataFrame(nan_values).reset_index()\r\n",
        "nans.columns = ['feature', 'count']\r\n",
        "nans = nans[nans['count'] != 0]\r\n",
        "del nan_values\r\n",
        "\r\n",
        "do = False\r\n",
        "if do:\r\n",
        "    i = 0\r\n",
        "    while i < 404:\r\n",
        "        sns.catplot(y ='feature', x=\"count\", kind=\"bar\", original_data=nans[i:i + 30])\r\n",
        "        i += 30\r\n",
        "    # ax.set(xscale=\"log\")\r\n",
        "    nans_sorted = nans['count'].sort_values()\r\n",
        "    plt.figure(figsize=(20,10))\r\n",
        "    nans_sorted.plot(kind = 'bar')\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxQwKpbtGTpT"
      },
      "source": [
        "del nans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei_skRLFGUxb"
      },
      "source": [
        "### MICE: Multivariate Imputation by Chained Equations\r\n",
        "\r\n",
        "Fa parte dei metodi a multiple imputation, ovvero quelli in cui i missing values sono generati più volte dal dataset. I dataset ottenuti vengono poi uniti, e i dati che rimpiazzeranno i missing values vengono scelti tramite qualche regola. \r\n",
        "\r\n",
        "### Fasi dei metodi a multiple imputation:\r\n",
        "1. imputation: calcolo in qualche modo il valore da assegnare al missing value;\r\n",
        "2. analisi dei risultati ottenuti dalle varie imputation;\r\n",
        "3. pooling: integrazione dei risultati nel dataset finale.\r\n",
        "\r\n",
        "### Fasi del metodo MICE:\r\n",
        "1. faccio una simple imputation per ogni missing value nel dataset;\r\n",
        "2. i missing value sono ripristinati;\r\n",
        "3. i valori osservati, cioè quelli generati dalle simple imputations, sono regressed dalle altre variabili nell'imputation model. \r\n",
        "4. i missing values sono quindi rimpiazzati con i valori predetti dal modello di regressione;\r\n",
        "5. gli step dal 2 al 4 vengono ripetuti per ogni variabile che ha missing values. La ripetizione di questi step costisuisce un ciclo. Alla fine di un ciclo i missing values sono stati rimpiazzati dai valori predetti dalla regressione che riflette le relazioni osservate nel dati;\r\n",
        "6. gli step dal 2 al 4 sono ripetuti per un numero fissato di cicli, e ad ogni ciclo i valori predetti vengono uppati. Alla fine di questi cicli i valori finali predetti andranno nel dataset. \r\n",
        "\r\n",
        "Di solito si effettuano dieci cicli. \r\n",
        "Idea: alla fine dei cicli, la distribuzione dei parametri che fornisce i valori predetti, deve convergere e diventare stabile. \r\n",
        "\r\n",
        "**Descrizione da scikit-learn:** models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output y and the other feature columns are treated as inputs X. A regressor is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y. This is done for each feature in an iterative fashion, and then is repeated for max_iter imputation rounds. The results of the final imputation round are returned.\r\n",
        "\r\n",
        "Riferimenti: https://www.kaggle.com/residentmario/simple-techniques-for-missing-data-imputation\r\n",
        "\r\n",
        "IterativeImputer: https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html\r\n",
        "\r\n",
        "Uso IterativeImputer: https://www.geeksforgeeks.org/missing-data-imputation-with-fancyimpute/\r\n",
        "\r\n",
        "Note: MICE era prima presente in fancyimpute, ma poi è stato spostato in scikit-learn\r\n",
        "\r\n",
        "`split_by_day`: Funzione che splitta il `dataset` nel numero di `days` specificato. \r\n",
        "\r\n",
        "`select_col_by_nan`: seleziona le colonne del `dataset` se il numero di NaN che contengono è inferiore alla soglia `tresh`. \r\n",
        "\r\n",
        "`mice`: performa il MICE sul `dataset` specificato, considerando il numero di `days` specificato. \r\n",
        "\r\n",
        "Faccio l'IterativeImputer su un singolo giorno. -> L'algoritmo non riesce a raggiungere la convergenza se ci sono colonne con troppi NaN.\r\n",
        "Errore:\r\n",
        "\r\n",
        "`/opt/conda/lib/python3.7/site-packages/sklearn/impute/_iterative.py:670: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\r\n",
        "  \" reached.\", ConvergenceWarning)`\r\n",
        "  \r\n",
        "Per risolvere il problema seleziono le colonne con meno di 100k NaN e aumento max_iter a 40. Paper [M.A] sottolineava come fossero troppo ottimistiche le stime di max_iter 10 per casi con tanti missing values. \r\n",
        "\r\n",
        "Dopo che ho fatto l'imputing, il dataset viene riunito nuovamente e viene prodotto un csv. \r\n",
        "\r\n",
        "Riferimento paper [M.A]: https://drive.google.com/file/d/1aK5py9lvjo6WHFDIROS7frXG-f-O5TEN/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KywAUpRGWYN"
      },
      "source": [
        "if not os.path.isfile('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/fitted.csv'):\r\n",
        "# if os.path.isfile('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/fitted.csv'):\r\n",
        "    numerical_data = original_data.select_dtypes(include=['int64','float64'])\r\n",
        "    fitted = mice(numerical_data, days=182)\r\n",
        "    selected_cols = select_col_by_nan(numerical_data, tresh=100000)\r\n",
        "    print('Number of selected columns: ', len(selected_cols))\r\n",
        "    fitted1 = pd.concat(fitted)\r\n",
        "    fitted1\r\n",
        "    fitted1.to_csv(r'./fitted.csv')\r\n",
        "\r\n",
        "numerical_data = pd.read_csv('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/fitted.csv')\r\n",
        "numerical_data.drop(numerical_data.columns[numerical_data.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8bW8x9dH9kC"
      },
      "source": [
        "## Dataset split in numerical e categorical\r\n",
        "\r\n",
        "Seleziono le feature i cui dati sono numerici. \r\n",
        "\r\n",
        "https://pandas.pydata.org/pandas--docs/stable/reference/api/pandas.DataFrame.select_dtypes.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p352IRs9H_RK"
      },
      "source": [
        "num_data = numerical_data.drop(['isFraud','TransactionID','TransactionDT'], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_d9djnkIBj1"
      },
      "source": [
        "categorical_col = []\r\n",
        "categorical_col.append('TransactionID')\r\n",
        "categorical_col.append('isFraud')\r\n",
        "for col in original_data:\r\n",
        "  if col not in numerical_cols:\r\n",
        "    categorical_col.append(col)\r\n",
        "\r\n",
        "len(categorical_col)\r\n",
        "categorical_data = original_data[original_data.columns.intersection(categorical_col)]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}