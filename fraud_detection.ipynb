{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 0. Import e installazione librerie\n\nInstallo via pip seaborn nell'ultima versione disponibile. \nhttps://www.kaggle.com/product-feedback/186010\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install seaborn --upgrade","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.__version__","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f00914fe-0133-4dc8-8328-ada46990186d","_cell_guid":"536ef7b0-873a-41a4-97e0-9600a3b49601","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib\nimport matplotlib.pyplot as plt\n# import seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Import dataset e descrizione feature\n\n* TransactionDT: timedelta from a given reference datetime\n\n* TransactionAMT: transaction payment amount in USD\n\n* card4: mastercard, visa and other.\n\n* card6: credit or debit.\n\n* P_ and (R__) emaildomain: purchaser and recipient email domain. Categorical: gmail.com, hotmail.com and others.\n\nProductCD: product code, the product for each transaction. Categorical. \n\nC1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n\nD1-D15: timedelta, such as days between previous transaction, etc.\n\nM1-M9: match, such as names on card and address, etc. Categorical: T or F.  \n\nVxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n\n* DeviceType: type of device used, mobile or desktop.\n\n* DeviceInfo: info on device.\n\n* id_30: OS name and version.\n\n* id_31: browser name and version.\n\n* id_33: screen size.\n\nid_35 to 38: T or F.\n\nid_34: match_status:1 or 2.\n \nid_15 and 28: New or Found. Maybe Found if a customer is registered in Vesta database. \n\nid_12 and 16: Found or NotFound. \n\nid_27: Null of Found.\n\nVado ad importare il training set\n\nUnisco i due dataset tramite la funzione `merge`, che li unisce rispetto alla colonna `TransactionID`. \n\n`original_data` conterrà il dataset originale, senza rimpiazzamento dei missing values nè encoding delle features categoriche. \n\nFunzione `merge`: https://stackoverflow.com/questions/41463119/join-two-dataframes-on-common-column-in-python\n\n`to_csv`: https://datatofish.com/export-dataframe-to-csv/"},{"metadata":{"trusted":true},"cell_type":"code","source":"if not os.path.isfile('../input/original-datacsv/original_data.csv'):\n    data_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv')\n    data_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv')\n    # unisco i due dataframe\n    data = pd.merge(data_transaction, data_identity, left_on='TransactionID', right_on='TransactionID', how='left')\n    original_data = data\n\n    del data\n    del data_transaction\n    del data_identity\n\n    original_data.to_csv(r'./original_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_data = pd.read_csv('../input/original-datacsv/original_data.csv')\noriginal_data.drop(original_data.columns[original_data.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Missing values: MICE method\n\nSeleziono i NaN dal dataframe e li conto. \n\nCostruisco `nans`, che è un dataframe che contiene (feature, nan_totali). \n\nhttps://stackoverflow.com/questions/28503445/assigning-column-names-to-a-pandas-series\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"nan_values = original_data.isna().sum()\nnans = pd.DataFrame(nan_values).reset_index()\nnans.columns = ['feature', 'count']\nnans = nans[nans['count'] != 0]\ndel nan_values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotto i NaN contati per ogni feature. "},{"metadata":{"trusted":true},"cell_type":"code","source":"do = False\nif do:\n    i = 0\n    while i < 404:\n        sns.catplot(y ='feature', x=\"count\", kind=\"bar\", original_data=nans[i:i + 30])\n        i += 30\n    # ax.set(xscale=\"log\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Faccio il plot dei NaN avendo ordinato le features in modo crescente. (si può migliorare)"},{"metadata":{"trusted":true},"cell_type":"code","source":"do = False\nif do:\n    nans_sorted = nans['count'].sort_values()\n    plt.figure(figsize=(20,10))\n    nans_sorted.plot(kind = 'bar')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del nans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MICE\n\nFa parte dei metodi a multiple imputation, ovvero quelli in cui i missing values sono generati più volte dal dataset. I dataset ottenuti vengono poi uniti, e i dati che rimpiazzeranno i missing values vengono scelti tramite qualche regola. \n\n### Fasi dei metodi a multiple imputation:\n1. imputation: calcolo in qualche modo il valore da assegnare al missing value;\n2. analisi dei risultati ottenuti dalle varie imputation;\n3. pooling: integrazione dei risultati nel dataset finale.\n\n### Fasi del metodo MICE:\n1. faccio una simple imputation per ogni missing value nel dataset;\n2. i missing value sono ripristinati;\n3. i valori osservati, cioè quelli generati dalle simple imputations, sono regressed dalle altre variabili nell'imputation model. \n4. i missing values sono quindi rimpiazzati con i valori predetti dal modello di regressione;\n5. gli step dal 2 al 4 vengono ripetuti per ogni variabile che ha missing values. La ripetizione di questi step costisuisce un ciclo. Alla fine di un ciclo i missing values sono stati rimpiazzati dai valori predetti dalla regressione che riflette le relazioni osservate nel dati;\n6. gli step dal 2 al 4 sono ripetuti per un numero fissato di cicli, e ad ogni ciclo i valori predetti vengono uppati. Alla fine di questi cicli i valori finali predetti andranno nel dataset. \n\nDi solito si effettuano dieci cicli. \nIdea: alla fine dei cicli, la distribuzione dei parametri che fornisce i valori predetti, deve convergere e diventare stabile. \n\n**Descrizione da scikit-learn:** models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output y and the other feature columns are treated as inputs X. A regressor is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y. This is done for each feature in an iterative fashion, and then is repeated for max_iter imputation rounds. The results of the final imputation round are returned.\n\nRiferimenti: https://www.kaggle.com/residentmario/simple-techniques-for-missing-data-imputation\n\nIterativeImputer: https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html\n\nUso IterativeImputer: https://www.geeksforgeeks.org/missing-data-imputation-with-fancyimpute/\n\nNote: MICE era prima presente in fancyimpute, ma poi è stato spostato in scikit-learn"},{"metadata":{},"cell_type":"markdown","source":"`split_by_day`: Funzione che splitta il `dataset` nel numero di `days` specificato. \n\n`select_col_by_nan`: seleziona le colonne del `dataset` se il numero di NaN che contengono è inferiore alla soglia `tresh`. \n\n`mice`: performa il MICE sul `dataset` specificato, considerando il numero di `days` specificato. \n\n\nProvare a fare l'imputing sul dataset diviso in giorni per tutti i giorni su cui va il dataset (183), e poi riunirlo alla fine per avere il dataset completo senza missing values.\nSarebbe utile averlo tutto insieme per continuare l'analisi delle features numeriche e della significatività delle colonne col dataset completo. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_by_day(dataset, days): \n    \n    day = 86400 # secondi in un giorno\n\n    # indici per il loop\n    start = day\n    end = day * 2 \n\n    data_splitted = {} # dizionario che contiene i vari set splittati per giorno\n    # loop per riempire il dizionario\n    for i in range(days):\n        data_splitted[i] = dataset[(original_data['TransactionDT'] >= (start)) & (dataset['TransactionDT'] < (end - 1))]\n        start += day\n        end += day\n        \n    return data_splitted\n\ndef select_col_by_nan(dataset, tresh = 400000): # funzione per selezionare le colonne di un dataset in base al numero di NaN\n    cols = [] # lista che contiene le colonne con numero di NaN inferiore alla soglia \n    for col in dataset.columns:\n        if dataset[col].isna().sum() < tresh:\n            cols.append(col)\n    return cols\n\ndef mice(dataset, days, tresh = 400000): # funzione che performa il MICE sul dataset selezionato\n    from sklearn.experimental import enable_iterative_imputer\n    from sklearn.impute import IterativeImputer\n    \n    data_splitted = split_by_day(dataset, days) # dizionario con dataset splittato per giorno\n    cols = select_col_by_nan(dataset, tresh) # lista che contiene le colonne con meno nan della soglia\n            \n    fitted = {} # dizionario che contiene i dataset splittati con gli imputed values\n    for day in range(days): # faccio l'imputation su ogni dataset riguardante le transazioni giornaliere \n        subset = data_splitted[day][[col for col in dataset.columns if col in cols]]\n        imp = IterativeImputer(missing_values=np.nan, random_state=0, n_nearest_features=5)                          \n        imp.fit(subset)\n        subset = imp.transform(subset)\n        fitted_set = subset\n        fitted[day] = pd.DataFrame(fitted_set, columns = cols).round(2) # trasformo la matrice ottenuta in un dataframe \n\n    return fitted # ritorno il dizionario i cui elementi sono i dataset giornalieri con i valori imputed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Faccio l'IterativeImputer su un singolo giorno. -> L'algoritmo non riesce a raggiungere la convergenza se ci sono colonne con troppi NaN (>500k)\n\n`/opt/conda/lib/python3.7/site-packages/sklearn/impute/_iterative.py:670: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n  \" reached.\", ConvergenceWarning)`\n  \nPer risolvere il problema seleziono le colonne con meno di 400k NaN. "},{"metadata":{"trusted":true},"cell_type":"code","source":"if not os.path.isfile('../input/fittedcsv/fitted.csv'):\n\n    fitted = mice(numerical_data, days=182)\n    selected_cols = select_col_by_nan(numerical_data)\n    print('Number of selected columns: ', len(selected_cols))\n    fitted1 = pd.concat(fitted)\n    fitted1.to_csv(r'./fitted.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mice_data = pd.read_csv('../input/fittedcsv/fitted.csv')\nmice_data.drop(mice_data.columns[mice_data.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Dataset split in numerical e categorical\n\nSeleziono le feature i cui dati sono numerici. \n\nhttps://pandas.pydata.org/pandas--docs/stable/reference/api/pandas.DataFrame.select_dtypes.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_data = mice_data.select_dtypes(include=['int64','float64'])\n# numerical_data = original_data.select_dtypes(include=['int64','float64'])\nnum_data = numerical_data.drop(['isFraud','TransactionID','TransactionDT'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seleziono le features i cui valori sono categorici. "},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_col = []\ncategorical_col.append('TransactionID')\ncategorical_col.append('isFraud')\n\nfor col in original_data.columns:\n    if col not in numerical_data.columns:\n        categorical_col.append(col)\n    \ncategorical_data = original_data[categorical_col]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Exploratory analysis\n\nVedo su quanti giorni va il dataset e controllo la proporzione tra eventi fraudolenti e non. "},{"metadata":{"trusted":true},"cell_type":"code","source":"total_days = np.ceil(max(original_data['TransactionDT'])/(86400))\nprint('Dataset spans ', total_days, ' days')\n\nfraud = (original_data['isFraud'] == 1).sum()\nsafe = (original_data['isFraud'] == 0).sum()\nprint('Fraudolent events: ', fraud)\nprint('Safe events: ', safe)\nprint('Ratio Fraud/safe: ', fraud/(fraud + safe))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1. Analisi feature numeriche\n\nAnalizzo la distribuzione e la significatività statistica delle feature numeriche. \n\nBoxplot per confrontare la distribuzione delle feature numeriche tra transazioni fraudolente e non. \n\nRiferimento: https://stackoverflow.com/questions/62166292/seaborn-catplot-is-throwing-error-truth-value-is-ambiguous"},{"metadata":{"trusted":true},"cell_type":"code","source":"melted = []\nn_plot = 3\ntarget_col = \"isFraud\"\ncat_cols = num_data.columns[(num_data.dtypes == int) | (num_data.dtypes == float)]\n# for i in range(len(cat_cols)):\nfor i in range(n_plot):\n    melted.append(numerical_data.melt(id_vars=target_col,value_vars=cat_cols[i]))\n    g = sns.FacetGrid(melted[i], col='variable', sharex=False,col_wrap=1)\n#   https://stackoverflow.com/questions/35131798/tweaking-seaborn-boxplot\n    g.map_dataframe(sns.boxplot, x=\"isFraud\", y=\"value\", showfliers=False, hue = 'isFraud')\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizzo la differenza nella distribuzione di probabilità della feature a seconda della label. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://seaborn.pydata.org/generated/seaborn.histplot.html\nsns.histplot(data=numerical_data[['card1','isFraud']], x=\"card1\", hue=\"isFraud\", element=\"step\",\n    stat=\"density\", common_norm=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calcolo le medie e le deviazioni standard per poter calcolare la variabile t ed effettuare il test di Welch."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_stat(dataset, mean = False, std = False):\n    col_to_drop = ['TransactionAmt','TransactionID','TransactionDT']\n    if mean:\n        means = dataset.groupby(['isFraud']).mean()\n        means = means.drop(col_to_drop, axis = 1)\n        return means\n    if std:\n        stds = dataset.groupby(['isFraud']).std()\n        stds = stds.drop(col_to_drop, axis = 1)\n        return stds\n    else:\n        print('Specificy if you want the mean or std')\n    \ndef get_subFrame(dataset, safe = False, fraud = False):\n    col_to_drop = ['TransactionAmt','TransactionID','TransactionDT','isFraud']\n    if safe:\n        safe_dataset = dataset[dataset['isFraud']==0].drop(col_to_drop, axis = 1)\n        return safe_dataset\n    if fraud:\n        fraud_dataset = dataset[dataset['isFraud']==1].drop(col_to_drop, axis = 1)\n        return fraud_dataset\n    else:\n        print('Specificy if you want the safe or fraud rows')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"means = get_stat(numerical_data, mean=True)\nstds = get_stat(numerical_data, std=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"safe_numerical = get_subFrame(numerical_data, safe = True)\nfraud_numerical = get_subFrame(numerical_data, fraud = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Definisco le funzioni per poter effettuare il Welch test. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\n\ndef diff(df):\n    res = {}\n    for col in df.columns:\n        res[col] = df[col][0] - df[col][1]\n    return res\n\ndef s(df_safe, df_fraud):\n    res = {}\n    for col in df_safe.columns:\n        s0_2 = stds[col][0]**2\n        s1_2 = stds[col][1]**2\n        n0 = len(df_safe)\n        n1 = len(df_fraud)\n        res[col] = np.sqrt(s0_2 /n0 + s1_2 /n1)\n    return res\n\ndef t(mean, std, df_safe, df_fraud):\n    res = {}\n    for col in mean.columns:\n        res[col] = diff(mean)[col] / s(df_safe, df_fraud)[col]\n    return res\n\ndef v(df_safe, df_fraud):\n    res = {}\n    for col in df_safe.columns:\n        s0_2 = stds[col][0]**2\n        s1_2 = stds[col][1]**2\n        n0 = len(df_safe)\n        n1 = len(df_fraud)\n        v0 = n0 - 1\n        v1 = n1 - 1\n        v = ((s0_2 / n0 + s1_2 / n1)**2)/(s0_2**2/(n0**2 * v0) + s1_2**2/(n1**2 * v1))\n        res[col] = np.ceil(v)\n    return res\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Effettuo il Welch t-test https://en.wikipedia.org/wiki/Welch%27s_t-test\n\n$ t = \\frac{\\mu_{0} - \\mu_{1}}{\\sqrt{\\frac{s_{0}^{2}}{N_{0}} + \\frac{s_{1}^{2}}{N_{1}}}}$\n\n$ \\nu = \\frac{(\\frac{s_{0}^{2}}{N_{0}} + \\frac{s_{1}^{2}}{N_{1}})^{2}}{{\\frac{s_{0}^{4}}{N_{0}^{2}\\nu_{0}} + \\frac{s_{1}^{4}}{N_{1}^{2}\\nu_{1}}}}$\n \n\nt e dof sono due dizionari. t contiene i valori della variabile t, e dof i gradi di libertà. "},{"metadata":{"trusted":true},"cell_type":"code","source":"t_variable = t(means, stds, safe_numerical, fraud_numerical)\ndof = v(safe_numerical, fraud_numerical)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Una volta effettuato il test, scelgo il livello di significatività e vado a selezionare le feature con p-value superiore a tale livello. \n\nCalcolo del p-value: https://towardsdatascience.com/inferential-statistics-series-t-test-using-numpy-2718f8f9bf2f"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sig_cols(t_variable, dataset, liv_sign = 0.95):\n    p_value = {}\n    sig_cols = 0\n    num_sign_col = []\n#     num_col_not_sign = []\n\n    for col in dataset.columns:    \n        p_value[col] = 1 - stats.t.cdf(t_variable[col], df = dof[col])\n        if p_value[col] > liv_sign:\n            num_sign_col.append(col)\n            print('Feature ', col, 'has a pvalue of: ', p_value[col])\n            sig_cols += 1\n#         else:\n#             num_col_not_sign.append(col)\n#             print('Feature ', col, 'is below significancy level')\n    print(len(num_sign_col), ' significative columns on ', len(dataset.columns), 'total columns')\n    return num_sign_col","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`num_sign_col`: lista con le colonne numeriche significative tramite test di Welch e con missing values rimpiazzati tramite MICE. \n\nRiferimento scrittura file testo: https://stackoverflow.com/questions/899103/writing-a-list-to-a-file-with-python\n\nRiferimento lettura file testo: https://www.kite.com/python/answers/how-to-read-a-text-file-into-a-list-in-python\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"if not os.path.isfile('../input/num-sign-col-mice/num_sign_col_mice.txt'):\n    num_sign_col = sig_cols(t_variable, safe_numerical)\n    with open('num_sign_col_mice.txt', 'w') as f:\n        for item in num_sign_col:\n            f.write(\"%s \" % item)\n            \nfile = open(\"../input/num-sign-col-mice/num_sign_col_mice.txt\", \"r\")\nnum_sign_col = file.read()\nnum_sign_col = num_sign_col.split(\" \")\nfile.close()\nnum_sign_col.pop()\nnum_sign_col","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2. Analisi feature categoriche\nAnalizzo il comportamento delle feature categoriche. "},{"metadata":{},"cell_type":"markdown","source":"Istogrammi delle feature categoriche. "},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in categorical_col[3:10]:\n    ax = sns.catplot(data = categorical_data, x=col, hue='isFraud', kind = 'count')\n    ax.set(yscale=\"log\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inizializzo i dizionari che conterranno i conteggi e le frequenze relative a ciascuna feature. Inoltre elimino le colonne isFraud e TransactionID. "},{"metadata":{"trusted":true},"cell_type":"code","source":"count = {}\nfrequencies = {}\ndone = False\nif not done:\n    categorical_col.remove('isFraud')\n    categorical_col.remove('TransactionID')\n    done = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Riferimento `group_by`: https://stackoverflow.com/questions/42563209/how-to-count-subgroups-of-categorical-data-in-a-pandas-dataframe\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in categorical_col:\n    count[col] = categorical_data.groupby('isFraud')[col].value_counts().unstack(fill_value=0)\n    frequencies[col] = categorical_data.groupby('isFraud')[col].value_counts(normalize = True).unstack(fill_value=0)    \n#     print(frequencies[col], '\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test $\\chi^2$\nImporto la funzione `chi2_contingency` che permette di effettuare il test $\\chi^2$. \n\n$H_{0}$: le differenze tra le frequenze nel caso di transazioni fraudolente e non, non siano statisticamente significative, cioè che siano il frutto di noise nel dataset. \n\nRisultato: Se il p-value è inferiore al livello di significatività del test, allora la differenza tra le frequenze è significativa, cioè non è casuale. \n\nRiferimenti `chi2_contingency`: \n\n* https://www.geeksforgeeks.org/python-pearsons-chi-square-test/\n\n* https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_sign_cols(count, liv_sign = 0.95):\n    from scipy.stats import chi2_contingency \n\n    stat = {}\n    p = {}\n    dof = {}\n    expected = {}\n\n    # dizionari che contengono i nomi delle feature significative e non \n    cat_col_sign = []\n#     cat_col_not_sign = []\n\n    for col in count:\n        stat[col], p[col], dof[col], expected[col] = chi2_contingency(count[col]) \n        if p[col] < liv_sign:\n            cat_col_sign.append(col)\n            print('Feature ', col,' is significant \\t Chi square: ', stat[col], '\\t dof: ', dof[col], '\\n')\n#         else:\n#             cat_col_not_sign.append(col)\n#             print('Feature ', col,' is NOT significant \\t Chi square: ', stat[col], '\\t dof: ', dof[col], '\\n')\n\n    print('Number of significative features: ', len(cat_col_sign))\n    return cat_col_sign","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`cat_sign_col`: lista che contiene il nome delle colonne, con variabili categorihe, significative tramite test $\\chi^{2}$. "},{"metadata":{"trusted":true},"cell_type":"code","source":"if not os.path.isfile('../input/cat-sign-col/cat_sign_col.txt'):\n    cat_sign_col = get_sign_cols(count)\n    with open('cat_sign_col.txt', 'w') as f:\n        for item in cat_sign_col:\n            f.write(\"%s \" % item)\n            \nfile = open(\"../input/cat-sign-col/cat_sign_col.txt\", \"r\")\ncat_sign_col = file.read() # importo il file\ncat_sign_col = cat_sign_col.split(\" \") # le colonne sono separate dallo spazio\nfile.close() \ncat_sign_col.pop() # levo l'ultimo elemento che è vuoto","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Analisi delle correlazioni\n\n#### Funzioni:\n\n* `dropColNotSign(dataset, col_sign, not_ignore = None)`: funzione per eliminare le colonne non significative dal dataset. `not_ignore`: colonna da ritenere significativa anche se non presente nella lista `col_sign`. \n\n* `corr_matrix_plot(dataset, corr_matrix)`: funzione per plottare la matrice di correlazione. \n\n* `highest_correlations(corr_matrix, tresh = 0.8)`: restituisce le features con correlazione superiore alla soglia `tresh` specificata.\n\n* `corr_dict(corr)`: restituisce un dizionario i cui elementi sono le coppie di features correlate, con relativo valore di correlazione. \n\n#### Variabili: \n\n`col_sign` è una lista che conterrà le colonne significative, cioè quelle selezionate per portare avanti l'analisi. \n\n`corr_d`: dizionario con le coppie di features correlate e relativa correlazione. \n\n#### Riferimenti \n\n* Riferimento sul sort: https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\n\n- Riferimento `drop_duplicates`: https://stackoverflow.com/questions/30530663/how-to-select-distinct-across-multiple-data-frame-columns-in-pandas\n\n* Riferimento `corr_sorted[corr_sorted>tresh]`: https://stackoverflow.com/questions/32067054/remove-rows-of-zeros-from-a-pandas-series\n\n* Riferimento `corr_dict`: https://stackoverflow.com/questions/25929319/how-to-iterate-over-pandas-multiindex-dataframe-using-index\n\n* Riferimento `np.isnan`: https://numpy.org/doc/stable/reference/generated/numpy.isnan.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"def dropColNotSign(dataset, col_sign, not_ignore = None):\n    if not fraud:\n        for col in dataset.columns:\n            if col not in col_sign and col != not_ignore:\n                dataset = dataset.drop([col], axis=1)\n    return dataset\n\ndef corr_matrix_plot(dataset, corr_matrix):\n    f = plt.figure(figsize=(20, 20))\n    plt.matshow(corr_matrix, fignum=f.number)\n    plt.xticks(range(dataset.shape[1]), dataset.columns, fontsize=10, rotation=90)\n    plt.yticks(range(dataset.shape[1]), dataset.columns, fontsize=10)\n    cb = plt.colorbar()\n    cb.ax.tick_params(labelsize=14)\n    plt.title('Correlation Matrix', fontsize=16);\n    \ndef highest_correlations(corr_matrix, tresh = 0.8):\n    corr_matrix_abs = corr_matrix.abs()\n    corr_matrix_abs = corr_matrix_abs.unstack()\n    corr_sorted = corr_matrix_abs.sort_values(kind=\"quicksort\")\n    corr_sorted = corr_sorted[:-len(num_sign_col)] # levo i termini sulla diagonale\n    corr_sorted = corr_sorted.drop_duplicates() # levo i termini doppi\n    corr_sorted = corr_sorted[corr_sorted>tresh]\n    return corr_sorted\n\ndef corr_dict(corr):\n    corr_dict = {}\n    corr_list = []\n    for col1, df in corr.groupby(level=0):\n        for col2 in df:\n           if not np.isnan(corr.loc[col1,col2]):\n                corr_dict[col1,col2] = corr.loc[col1,col2]\n                corr_list.append([col1,col2])\n                corr_list.append([col2,col1])\n                \n    return corr_dict, corr_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_sign = num_sign_col + cat_sign_col # lista con le features significative\nprint('Number of significative features: ', len(col_sign))\ncorrelation_data = dropColNotSign(mice_data, num_sign_col) # dataset solo con le features significative\ncorrelation_data_fraud = dropColNotSign(mice_data, num_sign_col, not_ignore = 'isFraud') \ncorrelation_data_fraud = correlation_data_fraud[correlation_data_fraud['isFraud'] == 1 ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = correlation_data.corr()\ncorr_matrix_plot(correlation_data, corr_matrix)\nhighest_corr = highest_correlations(corr_matrix)\nhighest_corr= highest_corr.unstack(level = 1)\n\ncorr_matrix_fraud = correlation_data_fraud.corr()\ncorr_matrix_plot(correlation_data_fraud, corr_matrix_fraud)\nhighest_corr_fraud = highest_correlations(corr_matrix_fraud)\nhighest_corr_fraud = highest_corr_fraud.unstack(level = 1)\n\ncorr_d, corr_list = corr_dict(highest_corr)\ncorr_d_fraud, corr_list_fraud = corr_dict(highest_corr_fraud)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for item in corr_list_fraud:\n    if item not in corr_list:\n        print(item)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Riferimento `scatterplot`: https://seaborn.pydata.org/generated/seaborn.scatterplot.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(data=correlation_data[['C7','C12']], x=\"C7\", y=\"C12\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fare differenza delle correlazioni nel caso fraud e safe per vedere se spuntano correlazioni diverse. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.1 Train validation split"},{"metadata":{},"cell_type":"markdown","source":"Splitto ogni set corrispondente ad un giorno in train e validation set. \n\nIl test set non è ancora stato caricato. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train = {}\nX_val = {}\ny_train = {}\ny_val = {}\n\nfor i in range(n_set):\n    X_train[i], X_val[i], y_train[i], y_val[i] = train_test_split(X[i], y[i], test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Model selection\n\nPotrebbe essere utile andare a valutare le prestazioni del modello con e senza le feature ingegnerizzate Vxxx"},{"metadata":{},"cell_type":"markdown","source":"Provo ad usare AdaBoost su un singolo subset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://scikit-learn.org/stable/modules/ensemble.html\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\n# for i in range(n_set):\nclf = AdaBoostClassifier(n_estimators=100)\nscores = cross_val_score(clf, X_train[0], y_train[0], cv=5)\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# valuto le performance del modello\nclf.fit(X_train[0], y_train[0])\ny_pred = clf.predict(X_val[0])\ncm = confusion_matrix(y_val[0], y_pred)\ncm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prova del balanced random forest di imblearn. "},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# from sklearn.ensemble import AdaBoostClassifier\n# from sklearn.metrics import confusion_matrix\n# from sklearn.metrics import balanced_accuracy_score\n# from imblearn.ensemble import BalancedRandomForestClassifier\n# from imblearn.metrics import geometric_mean_score\n\n# brf = BalancedRandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n\n# brf.fit(X_train[i], y_train[i])\n# y_pred_brf = brf.predict(X_val[i])\n\n# print('Balanced Random Forest classifier performance:')\n# print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n#       .format(balanced_accuracy_score(y_val[i], y_pred_brf),\n#               geometric_mean_score(y_val[i], y_pred_brf)))\n# cm_brf = confusion_matrix(y_val[i], y_pred_brf)\n# cm_brf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prova di EasyEnsemble"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from imblearn.ensemble import EasyEnsembleClassifier\n# # Apply Easy Ensemble\n# ee = EasyEnsembleClassifier(n_estimators=5)\n# X_resampled, y_resampled = ee.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removed"},{"metadata":{},"cell_type":"markdown","source":"Rimpiazzo i NaN con il valore 0 (da rivedere con le tecniche per rimpiazzare i missing values. \n\nFaccio il one hot encoding per avere solamente feature numeriche. (anche questo può essere riconsiderato)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# elimino i nan andandoli a rimpiazzare con zero \n# data = data.replace(to_replace = np.nan, value = 0)\n\n# faccio il one hot encoding delle feature categoriche così da avere solo valori numerici\n# data = pd.get_dummies(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sec_in_day = 86400\n# # tempo da cui inizio a selezionare\n# start = sec_in_day - 1\n# # tempo in cui si conclude lo split \n# end = sec_in_day * 2 - 1\n# # creo un array che contiene i dati suddivisi per giorno\n# sub_data = []\n# # for i in range(int(total_days)):\n# for i in range(10):\n# #   per concatenare due condizioni devo delimitarle con le parentesi \n# #   https://stackoverflow.com/questions/31617845/how-to-select-rows-in-a-dataframe-between-two-values-in-python-pandas\n#     sub_data.append(data[(data['TransactionDT'] < end) & (data['TransactionDT'] > start)]);\n#     start += sec_in_day\n#     end += sec_in_day\n# # sub_data\n\n# fraud = []\n# safe = []\n# fraud_distr = []\n# for i in range(9):\n#     fraud_distr.append(sub_data[i][['TransactionDT','TransactionAmt','isFraud']]) \n#     fraud.append( fraud_distr[i][fraud_distr[i]['isFraud'] == 1])\n#     safe.append(fraud_distr[i][fraud_distr[i]['isFraud'] == 0])\n\n# x = []\n# y = []\n# # https://matplotlib.org/3.3.2/gallery/subplots_axes_and_figures/subplot.html#sphx-glr-gallery-subplots-axes-and-figures-subplot-py\n# # https://stackoverflow.com/questions/17210646/python-subplot-within-a-loop-first-panel-appears-in-wrong-position\n# fig, ax = plt.subplots(3, 3, figsize = (30,10))\n# fig.subplots_adjust(wspace = 1)\n# ax = ax.ravel()\n# # for loop per plottare i vari subplots\n# for i in range(9):\n#     x.append([fraud[i]['TransactionDT'], safe[i]['TransactionDT']])\n#     y.append([fraud[i]['TransactionAmt'], safe[i]['TransactionAmt']])\n\n#     fig.suptitle('Amount nel tempo')\n\n#     ax[i].plot(x[i][0], y[i][0], label = 'fraud')\n#     ax[i].plot( x[i][1],  y[i][1], alpha = 0.5, label = 'safe')\n# #     ax[i].set_title('Day')\n# #     ax[i].set_ylabel('amount')\n#     ax[i].set_xlabel('time (s)')\n\n# plt.show()\n\n# dev_info = original_data[['DeviceInfo', 'isFraud']]\n# dev_info = dev_info[dev_info['DeviceInfo'] != 0]\n# dev_info_fraud = dev_info[dev_info['isFraud'] == 1].value_counts(normalize = True)\n# dev_info_safe = dev_info[dev_info['isFraud'] == 0].value_counts(normalize = True)\n\n# dev_info_fraud[:10].plot(kind = 'bar', label = 'fraud', color = 'red');\n# dev_info_safe[:10].plot(kind = 'bar',  label='safe', color = 'green');\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Altro modo (peggiore) per fare l'iterative imputer su tutto il dataset\nFaccio l'iterative imputer su tutto il dataset però tenendo solo la colonna transactionAmt -> più veloce ma non è comunque corretto dal punto di vista concettuale. L'algoritmo stima i valori mancanti attraverso tutti gli altri valori contenuti nel dataset. Se gli dessi solo una colonna, potrebbe non essere sufficiente per valutare bene i missing values. \n\nCreo un dizionario (fitted_set) che conterrà le colonne con i valori stimati. \n\nFaccio un loop su tutte le colonne per stimare i valori. \n\nCon round setto il numero di decimali. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.round.html\n\nfitted è il dataframe che contiene i valori rimpiazzati, quindi senza NaN."},{"metadata":{"trusted":true},"cell_type":"code","source":"do = False\n\nif do:\n    # dizionario che contiene le colonne con i valori rimpiazzati\n    fitted_set = {}\n\n    for col in num_data.columns:\n        if col != 'TransactionAmt':\n            subset = num_data[['TransactionAmt',col]]\n            imp = IterativeImputer(missing_values=np.nan, random_state=0, n_nearest_features=5)                          \n            imp.fit(subset)\n            subset = imp.transform(subset)\n            fitted_set[col] = subset[:,1]\n\n    fitted = pd.DataFrame(fitted_set).round(2)\n    del fitted_set\n    # fitted","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlazione senza rimpiazzo dei missing values \nCerco correlazioni tra le features. \nVado ad eliminare le colonne con troppi zeri che sono meno informative. "},{"metadata":{"trusted":true},"cell_type":"code","source":"do = False \nif do: \n    data_splitted = split_by_day(original_data, 1)\n\n    # dizionario che contengono la matrice con i feature vector\n    X = {}\n    # dizionario che contiene le label\n    y = {}\n\n    # popolo X e y\n    for i in range(n_set):\n        X[i] = data_splitted[i].drop(['isFraud'], axis = 1)\n        y[i] = data_splitted[i]['isFraud']\n\n    # dizionario con i vari dataset per la correlazione\n    X_correlation = {}\n    thresh = 2000\n    sub_dataset = 0\n    # https://stackoverflow.com/questions/46628253/deleting-dataframe-column-in-pandas-based-on-value\n    # https://stackoverflow.com/questions/53550988/count-occurrences-of-false-or-true-in-a-column-in-pandas\n    X_correlation[sub_dataset] = X[sub_dataset].loc[:, ((X[sub_dataset] != 0).sum() > thresh)]\n    # X_correlation[sub_dataset]\n    \n    # https://stackoverflow.com/questions/29432629/plot-correlation-matrix-using-pandas\n    f = plt.figure(figsize=(40, 40))\n    plt.matshow(X_correlation[sub_dataset].corr(), fignum=f.number)\n    plt.xticks(range(X_correlation[sub_dataset].shape[1]), X_correlation[sub_dataset].columns, fontsize=10, rotation=90)\n    plt.yticks(range(X_correlation[sub_dataset].shape[1]), X_correlation[sub_dataset].columns, fontsize=10)\n    cb = plt.colorbar()\n    cb.ax.tick_params(labelsize=14)\n    plt.title('Correlation Matrix', fontsize=16);","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}