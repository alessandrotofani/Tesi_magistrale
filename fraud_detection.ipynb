{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "fraud_detection.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandrotofani/Tesi_magistrale/blob/master/fraud_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1vlfmJ5X0h4"
      },
      "source": [
        "# 0. Import e installazione librerie\n",
        "\n",
        "Bisogna aggiungere il drive dove sono i dataset. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f00914fe-0133-4dc8-8328-ada46990186d",
        "_cell_guid": "536ef7b0-873a-41a4-97e0-9600a3b49601",
        "trusted": true,
        "_kg_hide-output": true,
        "_kg_hide-input": true,
        "id": "dFu77X_AX0iH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e934be2d-ca4f-45f6-e76a-e612426c0aac"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "for dirname, _, filenames in os.walk('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/train_transaction.csv\n",
            "/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/train_identity.csv\n",
            "/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/original_data.csv\n",
            "/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/num_sign_col.txt\n",
            "/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/cat_sign_col.txt\n",
            "/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/num_sign_col_mice.txt\n",
            "/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/fitted.csv\n",
            "/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/fitted_old/fitted.csv\n",
            "/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/fitted_old/readme.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO46rSRKX0iI"
      },
      "source": [
        "# 1. Import dataset e descrizione feature\n",
        "\n",
        "* TransactionDT: timedelta from a given reference datetime\n",
        "\n",
        "* TransactionAMT: transaction payment amount in USD\n",
        "\n",
        "* card4: mastercard, visa and other.\n",
        "\n",
        "* card6: credit or debit.\n",
        "\n",
        "* P_ and (R__) emaildomain: purchaser and recipient email domain. Categorical: gmail.com, hotmail.com and others.\n",
        "\n",
        "ProductCD: product code, the product for each transaction. Categorical. \n",
        "\n",
        "C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n",
        "\n",
        "D1-D15: timedelta, such as days between previous transaction, etc.\n",
        "\n",
        "M1-M9: match, such as names on card and address, etc. Categorical: T or F.  \n",
        "\n",
        "Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n",
        "\n",
        "* DeviceType: type of device used, mobile or desktop.\n",
        "\n",
        "* DeviceInfo: info on device.\n",
        "\n",
        "* id_30: OS name and version.\n",
        "\n",
        "* id_31: browser name and version.\n",
        "\n",
        "* id_33: screen size.\n",
        "\n",
        "id_35 to 38: T or F.\n",
        "\n",
        "id_34: match_status:1 or 2.\n",
        " \n",
        "id_15 and 28: New or Found. Maybe Found if a customer is registered in Vesta database. \n",
        "\n",
        "id_12 and 16: Found or NotFound. \n",
        "\n",
        "id_27: Null of Found.\n",
        "\n",
        "Vado ad importare il training set\n",
        "\n",
        "Unisco i due dataset tramite la funzione `merge`, che li unisce rispetto alla colonna `TransactionID`. \n",
        "\n",
        "`original_data` conterrà il dataset originale, senza rimpiazzamento dei missing values nè encoding delle features categoriche. \n",
        "\n",
        "Funzione `merge`: https://stackoverflow.com/questions/41463119/join-two-dataframes-on-common-column-in-python\n",
        "\n",
        "`to_csv`: https://datatofish.com/export-dataframe-to-csv/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Di5qRzKVX0iJ"
      },
      "source": [
        "if not os.path.isfile('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/original_data.csv'):\n",
        "    data_identity = pd.read_csv('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/train_identity.csv')\n",
        "    data_transaction = pd.read_csv('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/train_transaction.csv')\n",
        "    # unisco i due dataframe\n",
        "    data = pd.merge(data_transaction, data_identity, left_on='TransactionID', right_on='TransactionID', how='left')\n",
        "    original_data = data\n",
        "\n",
        "    del data\n",
        "    del data_transaction\n",
        "    del data_identity\n",
        "\n",
        "    original_data.to_csv(r'./original_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2pSa0wWuX0iJ"
      },
      "source": [
        "original_data = pd.read_csv('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/original_data.csv')\n",
        "original_data.drop(original_data.columns[original_data.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK23PjHEX0iK"
      },
      "source": [
        "## 1.1 Missing values: MICE method\n",
        "\n",
        "Seleziono i NaN dal dataframe e li conto. \n",
        "\n",
        "Costruisco `nans`, che è un dataframe che contiene (feature, nan_totali). \n",
        "\n",
        "https://stackoverflow.com/questions/28503445/assigning-column-names-to-a-pandas-series\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BGsh5GpjX0iK"
      },
      "source": [
        "nan_values = original_data.isna().sum()\n",
        "nans = pd.DataFrame(nan_values).reset_index()\n",
        "nans.columns = ['feature', 'count']\n",
        "nans = nans[nans['count'] != 0]\n",
        "del nan_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqr5NcyUX0iK"
      },
      "source": [
        "Plotto i NaN contati per ogni feature. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3NfrK2SnX0iL"
      },
      "source": [
        "do = False\n",
        "if do:\n",
        "    i = 0\n",
        "    while i < 404:\n",
        "        sns.catplot(y ='feature', x=\"count\", kind=\"bar\", original_data=nans[i:i + 30])\n",
        "        i += 30\n",
        "    # ax.set(xscale=\"log\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIRwIcCbX0iL"
      },
      "source": [
        "Faccio il plot dei NaN avendo ordinato le features in modo crescente. (si può migliorare)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2gyymE5wX0iL"
      },
      "source": [
        "do = False\n",
        "if do:\n",
        "    nans_sorted = nans['count'].sort_values()\n",
        "    plt.figure(figsize=(20,10))\n",
        "    nans_sorted.plot(kind = 'bar')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pyO3HOD_X0iM"
      },
      "source": [
        "del nans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO5WFh7JX0iM"
      },
      "source": [
        "## MICE: Multivariate Imputation by Chained Equations\n",
        "\n",
        "Fa parte dei metodi a multiple imputation, ovvero quelli in cui i missing values sono generati più volte dal dataset. I dataset ottenuti vengono poi uniti, e i dati che rimpiazzeranno i missing values vengono scelti tramite qualche regola. \n",
        "\n",
        "### Fasi dei metodi a multiple imputation:\n",
        "1. imputation: calcolo in qualche modo il valore da assegnare al missing value;\n",
        "2. analisi dei risultati ottenuti dalle varie imputation;\n",
        "3. pooling: integrazione dei risultati nel dataset finale.\n",
        "\n",
        "### Fasi del metodo MICE:\n",
        "1. faccio una simple imputation per ogni missing value nel dataset;\n",
        "2. i missing value sono ripristinati;\n",
        "3. i valori osservati, cioè quelli generati dalle simple imputations, sono regressed dalle altre variabili nell'imputation model. \n",
        "4. i missing values sono quindi rimpiazzati con i valori predetti dal modello di regressione;\n",
        "5. gli step dal 2 al 4 vengono ripetuti per ogni variabile che ha missing values. La ripetizione di questi step costisuisce un ciclo. Alla fine di un ciclo i missing values sono stati rimpiazzati dai valori predetti dalla regressione che riflette le relazioni osservate nel dati;\n",
        "6. gli step dal 2 al 4 sono ripetuti per un numero fissato di cicli, e ad ogni ciclo i valori predetti vengono uppati. Alla fine di questi cicli i valori finali predetti andranno nel dataset. \n",
        "\n",
        "Di solito si effettuano dieci cicli. \n",
        "Idea: alla fine dei cicli, la distribuzione dei parametri che fornisce i valori predetti, deve convergere e diventare stabile. \n",
        "\n",
        "**Descrizione da scikit-learn:** models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output y and the other feature columns are treated as inputs X. A regressor is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y. This is done for each feature in an iterative fashion, and then is repeated for max_iter imputation rounds. The results of the final imputation round are returned.\n",
        "\n",
        "Riferimenti: https://www.kaggle.com/residentmario/simple-techniques-for-missing-data-imputation\n",
        "\n",
        "IterativeImputer: https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html\n",
        "\n",
        "Uso IterativeImputer: https://www.geeksforgeeks.org/missing-data-imputation-with-fancyimpute/\n",
        "\n",
        "Note: MICE era prima presente in fancyimpute, ma poi è stato spostato in scikit-learn\n",
        "\n",
        "`split_by_day`: Funzione che splitta il `dataset` nel numero di `days` specificato. \n",
        "\n",
        "`select_col_by_nan`: seleziona le colonne del `dataset` se il numero di NaN che contengono è inferiore alla soglia `tresh`. \n",
        "\n",
        "`mice`: performa il MICE sul `dataset` specificato, considerando il numero di `days` specificato. \n",
        "\n",
        "\n",
        "Provare a fare l'imputing sul dataset diviso in giorni per tutti i giorni su cui va il dataset (183), e poi riunirlo alla fine per avere il dataset completo senza missing values.\n",
        "Sarebbe utile averlo tutto insieme per continuare l'analisi delle features numeriche e della significatività delle colonne col dataset completo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "b9XkaROpX0iN"
      },
      "source": [
        "def split_by_day(dataset, days): \n",
        "    \n",
        "    day = 86400 # secondi in un giorno\n",
        "\n",
        "    # indici per il loop\n",
        "    start = day\n",
        "    end = day * 2 \n",
        "\n",
        "    data_splitted = {} # dizionario che contiene i vari set splittati per giorno\n",
        "    # loop per riempire il dizionario\n",
        "    for i in range(days):\n",
        "        data_splitted[i] = dataset[(original_data['TransactionDT'] >= (start)) & (dataset['TransactionDT'] < (end - 1))]\n",
        "        start += day\n",
        "        end += day\n",
        "        \n",
        "    return data_splitted\n",
        "\n",
        "def select_col_by_nan(dataset, tresh = 400000): # funzione per selezionare le colonne di un dataset in base al numero di NaN\n",
        "    cols = [] # lista che contiene le colonne con numero di NaN inferiore alla soglia \n",
        "    for col in dataset.columns:\n",
        "        if dataset[col].isna().sum() < tresh:\n",
        "            cols.append(col)\n",
        "    return cols\n",
        "\n",
        "def mice(dataset, days, tresh = 100000): # funzione che performa il MICE sul dataset selezionato\n",
        "    from sklearn.experimental import enable_iterative_imputer\n",
        "    from sklearn.impute import IterativeImputer\n",
        "    \n",
        "    data_splitted = split_by_day(dataset, days) # dizionario con dataset splittato per giorno\n",
        "    cols = select_col_by_nan(dataset, tresh) # lista che contiene le colonne con meno nan della soglia\n",
        "            \n",
        "    fitted = {} # dizionario che contiene i dataset splittati con gli imputed values\n",
        "    for day in range(days): # faccio l'imputation su ogni dataset riguardante le transazioni giornaliere \n",
        "        subset = data_splitted[day][[col for col in dataset.columns if col in cols]]\n",
        "        imp = IterativeImputer(missing_values=np.nan, random_state=0, n_nearest_features=5, max_iter = 40)                          \n",
        "        imp.fit(subset)\n",
        "        subset = imp.transform(subset)\n",
        "        fitted_set = subset\n",
        "        fitted[day] = pd.DataFrame(fitted_set, columns = cols).round(2) # trasformo la matrice ottenuta in un dataframe \n",
        "\n",
        "    return fitted # ritorno il dizionario i cui elementi sono i dataset giornalieri con i valori imputed\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBXxTzukX0iN"
      },
      "source": [
        "Faccio l'IterativeImputer su un singolo giorno. -> L'algoritmo non riesce a raggiungere la convergenza se ci sono colonne con troppi NaN (>500k)\n",
        "\n",
        "`/opt/conda/lib/python3.7/site-packages/sklearn/impute/_iterative.py:670: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
        "  \" reached.\", ConvergenceWarning)`\n",
        "  \n",
        "Per risolvere il problema seleziono le colonne con meno di 100k NaN e aumento max_iter a 40.\n",
        "\n",
        "Paper [L] sottolineava come fossero troppo ottimistiche le stime di max_iter 10 per casi con tanti missing values. \n",
        "\n",
        "Paper [L]: https://drive.google.com/file/d/1aK5py9lvjo6WHFDIROS7frXG-f-O5TEN/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bH1BtFR8X0iO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f275561-009b-44a0-feca-b1c49600562c"
      },
      "source": [
        "if not os.path.isfile('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/fitted.csv'):\n",
        "# if os.path.isfile('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/fitted.csv'):\n",
        "    numerical_data = original_data.select_dtypes(include=['int64','float64'])\n",
        "    fitted = mice(numerical_data, days=182)\n",
        "    selected_cols = select_col_by_nan(numerical_data)\n",
        "    print('Number of selected columns: ', len(selected_cols))\n",
        "    fitted1 = pd.concat(fitted)\n",
        "    fitted1\n",
        "    fitted1.to_csv(r'./fitted.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of selected columns:  213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WZVs-PYpX0iP"
      },
      "source": [
        "mice_data = pd.read_csv('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/fitted.csv')\n",
        "mice_data.drop(mice_data.columns[mice_data.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsvSZSU2X0iP"
      },
      "source": [
        "## 1.2 Dataset split in numerical e categorical\n",
        "\n",
        "Seleziono le feature i cui dati sono numerici. \n",
        "\n",
        "https://pandas.pydata.org/pandas--docs/stable/reference/api/pandas.DataFrame.select_dtypes.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zLXTTpuEX0iP"
      },
      "source": [
        "numerical_data = mice_data.select_dtypes(include=['int64','float64'])\n",
        "# numerical_data = original_data.select_dtypes(include=['int64','float64'])\n",
        "num_data = numerical_data.drop(['isFraud','TransactionID','TransactionDT'], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOutaZilX0iP"
      },
      "source": [
        "Seleziono le features i cui valori sono categorici. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3s7WwlabX0iQ"
      },
      "source": [
        "categorical_col = []\n",
        "categorical_col.append('TransactionID')\n",
        "categorical_col.append('isFraud')\n",
        "\n",
        "for col in original_data.columns:\n",
        "    if col not in numerical_data.columns:\n",
        "        categorical_col.append(col)\n",
        "    \n",
        "categorical_data = original_data[categorical_col]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnCQGfcNX0iQ"
      },
      "source": [
        "# 2. Exploratory analysis\n",
        "\n",
        "Vedo su quanti giorni va il dataset e controllo la proporzione tra eventi fraudolenti e non. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2rCud_geX0iQ"
      },
      "source": [
        "total_days = np.ceil(max(original_data['TransactionDT'])/(86400))\n",
        "print('Dataset spans ', total_days, ' days')\n",
        "\n",
        "fraud = (original_data['isFraud'] == 1).sum()\n",
        "safe = (original_data['isFraud'] == 0).sum()\n",
        "print('Fraudolent events: ', fraud)\n",
        "print('Safe events: ', safe)\n",
        "print('Ratio Fraud/safe: ', fraud/(fraud + safe))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZrGRCHSX0iQ"
      },
      "source": [
        "## 2.1. Analisi feature numeriche\n",
        "\n",
        "Analizzo la distribuzione e la significatività statistica delle feature numeriche. \n",
        "\n",
        "Boxplot per confrontare la distribuzione delle feature numeriche tra transazioni fraudolente e non. \n",
        "\n",
        "Riferimento: https://stackoverflow.com/questions/62166292/seaborn-catplot-is-throwing-error-truth-value-is-ambiguous"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gTSHgzyKX0iR"
      },
      "source": [
        "melted = []\n",
        "n_plot = 3\n",
        "target_col = \"isFraud\"\n",
        "cat_cols = num_data.columns[(num_data.dtypes == int) | (num_data.dtypes == float)]\n",
        "# for i in range(len(cat_cols)):\n",
        "for i in range(n_plot):\n",
        "    melted.append(numerical_data.melt(id_vars=target_col,value_vars=cat_cols[i]))\n",
        "    g = sns.FacetGrid(melted[i], col='variable', sharex=False,col_wrap=1)\n",
        "#   https://stackoverflow.com/questions/35131798/tweaking-seaborn-boxplot\n",
        "    g.map_dataframe(sns.boxplot, x=\"isFraud\", y=\"value\", showfliers=False, hue = 'isFraud')\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuKwvv9RX0iR"
      },
      "source": [
        "Visualizzo la differenza nella distribuzione di probabilità della feature a seconda della label. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "sCyymWUeX0iS"
      },
      "source": [
        "# https://seaborn.pydata.org/generated/seaborn.histplot.html\n",
        "sns.histplot(data=numerical_data[['card1','isFraud']], x=\"card1\", hue=\"isFraud\", element=\"step\",\n",
        "    stat=\"density\", common_norm=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H08ltjIxX0iS"
      },
      "source": [
        "Calcolo le medie e le deviazioni standard per poter calcolare la variabile t ed effettuare il test di Welch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4rL4pRMzX0iS"
      },
      "source": [
        "def get_stat(dataset, mean = False, std = False):\n",
        "    col_to_drop = ['TransactionAmt','TransactionID','TransactionDT']\n",
        "    if mean:\n",
        "        means = dataset.groupby(['isFraud']).mean()\n",
        "        means = means.drop(col_to_drop, axis = 1)\n",
        "        return means\n",
        "    if std:\n",
        "        stds = dataset.groupby(['isFraud']).std()\n",
        "        stds = stds.drop(col_to_drop, axis = 1)\n",
        "        return stds\n",
        "    else:\n",
        "        print('Specificy if you want the mean or std')\n",
        "    \n",
        "def get_subFrame(dataset, safe = False, fraud = False):\n",
        "    col_to_drop = ['TransactionAmt','TransactionID','TransactionDT','isFraud']\n",
        "    if safe:\n",
        "        safe_dataset = dataset[dataset['isFraud']==0].drop(col_to_drop, axis = 1)\n",
        "        return safe_dataset\n",
        "    if fraud:\n",
        "        fraud_dataset = dataset[dataset['isFraud']==1].drop(col_to_drop, axis = 1)\n",
        "        return fraud_dataset\n",
        "    else:\n",
        "        print('Specificy if you want the safe or fraud rows')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "khIj6LE6X0iS"
      },
      "source": [
        "means = get_stat(numerical_data, mean=True)\n",
        "stds = get_stat(numerical_data, std=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JmfbDDB8X0iT"
      },
      "source": [
        "safe_numerical = get_subFrame(numerical_data, safe = True)\n",
        "fraud_numerical = get_subFrame(numerical_data, fraud = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt-rW_UiX0iT"
      },
      "source": [
        "Definisco le funzioni per poter effettuare il Welch test. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6CGV9DkCX0iT"
      },
      "source": [
        "from scipy import stats\n",
        "\n",
        "def diff(df):\n",
        "    res = {}\n",
        "    for col in df.columns:\n",
        "        res[col] = df[col][0] - df[col][1]\n",
        "    return res\n",
        "\n",
        "def s(df_safe, df_fraud):\n",
        "    res = {}\n",
        "    for col in df_safe.columns:\n",
        "        s0_2 = stds[col][0]**2\n",
        "        s1_2 = stds[col][1]**2\n",
        "        n0 = len(df_safe)\n",
        "        n1 = len(df_fraud)\n",
        "        res[col] = np.sqrt(s0_2 /n0 + s1_2 /n1)\n",
        "    return res\n",
        "\n",
        "def t(mean, std, df_safe, df_fraud):\n",
        "    res = {}\n",
        "    for col in mean.columns:\n",
        "        res[col] = diff(mean)[col] / s(df_safe, df_fraud)[col]\n",
        "    return res\n",
        "\n",
        "def v(df_safe, df_fraud):\n",
        "    res = {}\n",
        "    for col in df_safe.columns:\n",
        "        s0_2 = stds[col][0]**2\n",
        "        s1_2 = stds[col][1]**2\n",
        "        n0 = len(df_safe)\n",
        "        n1 = len(df_fraud)\n",
        "        v0 = n0 - 1\n",
        "        v1 = n1 - 1\n",
        "        v = ((s0_2 / n0 + s1_2 / n1)**2)/(s0_2**2/(n0**2 * v0) + s1_2**2/(n1**2 * v1))\n",
        "        res[col] = np.ceil(v)\n",
        "    return res\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W5WpSQ_X0iT"
      },
      "source": [
        "Effettuo il Welch t-test https://en.wikipedia.org/wiki/Welch%27s_t-test\n",
        "\n",
        "$ t = \\frac{\\mu_{0} - \\mu_{1}}{\\sqrt{\\frac{s_{0}^{2}}{N_{0}} + \\frac{s_{1}^{2}}{N_{1}}}}$\n",
        "\n",
        "$ \\nu = \\frac{(\\frac{s_{0}^{2}}{N_{0}} + \\frac{s_{1}^{2}}{N_{1}})^{2}}{{\\frac{s_{0}^{4}}{N_{0}^{2}\\nu_{0}} + \\frac{s_{1}^{4}}{N_{1}^{2}\\nu_{1}}}}$\n",
        " \n",
        "\n",
        "t e dof sono due dizionari. t contiene i valori della variabile t, e dof i gradi di libertà. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2ppsws1qX0iT"
      },
      "source": [
        "t_variable = t(means, stds, safe_numerical, fraud_numerical)\n",
        "dof = v(safe_numerical, fraud_numerical)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g2pdy0CX0iU"
      },
      "source": [
        "Una volta effettuato il test, scelgo il livello di significatività e vado a selezionare le feature con p-value superiore a tale livello. \n",
        "\n",
        "Calcolo del p-value: https://towardsdatascience.com/inferential-statistics-series-t-test-using-numpy-2718f8f9bf2f"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mPy9G4vyX0iU"
      },
      "source": [
        "def sig_cols(t_variable, dataset, liv_sign = 0.95):\n",
        "    p_value = {}\n",
        "    sig_cols = 0\n",
        "    num_sign_col = []\n",
        "#     num_col_not_sign = []\n",
        "\n",
        "    for col in dataset.columns:    \n",
        "        p_value[col] = 1 - stats.t.cdf(t_variable[col], df = dof[col])\n",
        "        if p_value[col] > liv_sign:\n",
        "            num_sign_col.append(col)\n",
        "            print('Feature ', col, 'has a pvalue of: ', p_value[col])\n",
        "            sig_cols += 1\n",
        "#         else:\n",
        "#             num_col_not_sign.append(col)\n",
        "#             print('Feature ', col, 'is below significancy level')\n",
        "    print(len(num_sign_col), ' significative columns on ', len(dataset.columns), 'total columns')\n",
        "    return num_sign_col"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_99iR6aCX0iU"
      },
      "source": [
        "`num_sign_col`: lista con le colonne numeriche significative tramite test di Welch e con missing values rimpiazzati tramite MICE. \n",
        "\n",
        "Riferimento scrittura file testo: https://stackoverflow.com/questions/899103/writing-a-list-to-a-file-with-python\n",
        "\n",
        "Riferimento lettura file testo: https://www.kite.com/python/answers/how-to-read-a-text-file-into-a-list-in-python\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fgZTloZfX0iU"
      },
      "source": [
        "if not os.path.isfile('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/num_sign_col_mice.txt'):\n",
        "    num_sign_col = sig_cols(t_variable, safe_numerical)\n",
        "    with open('num_sign_col_mice.txt', 'w') as f:\n",
        "        for item in num_sign_col:\n",
        "            f.write(\"%s \" % item)\n",
        "            \n",
        "file = open(\"/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/num_sign_col_mice.txt\", \"r\")\n",
        "num_sign_col = file.read()\n",
        "num_sign_col = num_sign_col.split(\" \")\n",
        "file.close()\n",
        "num_sign_col.pop()\n",
        "num_sign_col"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RalFheDTX0iU"
      },
      "source": [
        "## 2.2. Analisi feature categoriche\n",
        "Analizzo il comportamento delle feature categoriche. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzOTjLjLX0iU"
      },
      "source": [
        "Istogrammi delle feature categoriche. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TPbb_RSWX0iV"
      },
      "source": [
        "for col in categorical_col[3:10]:\n",
        "    ax = sns.catplot(data = categorical_data, x=col, hue='isFraud', kind = 'count')\n",
        "    ax.set(yscale=\"log\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9_21ZQ6X0iV"
      },
      "source": [
        "Inizializzo i dizionari che conterranno i conteggi e le frequenze relative a ciascuna feature. Inoltre elimino le colonne isFraud e TransactionID. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2Uwv4xzzX0iV"
      },
      "source": [
        "count = {}\n",
        "frequencies = {}\n",
        "done = False\n",
        "if not done:\n",
        "    categorical_col.remove('isFraud')\n",
        "    categorical_col.remove('TransactionID')\n",
        "    done = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BofFJye0X0iV"
      },
      "source": [
        "Riferimento `group_by`: https://stackoverflow.com/questions/42563209/how-to-count-subgroups-of-categorical-data-in-a-pandas-dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mffLKunyX0iV"
      },
      "source": [
        "for col in categorical_col:\n",
        "    count[col] = categorical_data.groupby('isFraud')[col].value_counts().unstack(fill_value=0)\n",
        "    frequencies[col] = categorical_data.groupby('isFraud')[col].value_counts(normalize = True).unstack(fill_value=0)    \n",
        "#     print(frequencies[col], '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3EIw3JkX0iW"
      },
      "source": [
        "## Test $\\chi^2$\n",
        "Importo la funzione `chi2_contingency` che permette di effettuare il test $\\chi^2$. \n",
        "\n",
        "$H_{0}$: le differenze tra le frequenze nel caso di transazioni fraudolente e non, non siano statisticamente significative, cioè che siano il frutto di noise nel dataset. \n",
        "\n",
        "Risultato: Se il p-value è inferiore al livello di significatività del test, allora la differenza tra le frequenze è significativa, cioè non è casuale. \n",
        "\n",
        "Riferimenti `chi2_contingency`: \n",
        "\n",
        "* https://www.geeksforgeeks.org/python-pearsons-chi-square-test/\n",
        "\n",
        "* https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "T1tI--uyX0iW"
      },
      "source": [
        "def get_sign_cols(count, liv_sign = 0.95):\n",
        "    from scipy.stats import chi2_contingency \n",
        "\n",
        "    stat = {}\n",
        "    p = {}\n",
        "    dof = {}\n",
        "    expected = {}\n",
        "\n",
        "    # dizionari che contengono i nomi delle feature significative e non \n",
        "    cat_col_sign = []\n",
        "#     cat_col_not_sign = []\n",
        "\n",
        "    for col in count:\n",
        "        stat[col], p[col], dof[col], expected[col] = chi2_contingency(count[col]) \n",
        "        if p[col] < liv_sign:\n",
        "            cat_col_sign.append(col)\n",
        "            print('Feature ', col,' is significant \\t Chi square: ', stat[col], '\\t dof: ', dof[col], '\\n')\n",
        "#         else:\n",
        "#             cat_col_not_sign.append(col)\n",
        "#             print('Feature ', col,' is NOT significant \\t Chi square: ', stat[col], '\\t dof: ', dof[col], '\\n')\n",
        "\n",
        "    print('Number of significative features: ', len(cat_col_sign))\n",
        "    return cat_col_sign"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whoWed2UX0iW"
      },
      "source": [
        "`cat_sign_col`: lista che contiene il nome delle colonne, con variabili categorihe, significative tramite test $\\chi^{2}$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "egi6GxAEX0iW"
      },
      "source": [
        "if not os.path.isfile('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/cat_sign_col.txt'):\n",
        "    cat_sign_col = get_sign_cols(count)\n",
        "    with open('cat_sign_col.txt', 'w') as f:\n",
        "        for item in cat_sign_col:\n",
        "            f.write(\"%s \" % item)\n",
        "            \n",
        "file = open(\"/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/cat_sign_col.txt\", \"r\")\n",
        "cat_sign_col = file.read() # importo il file\n",
        "cat_sign_col = cat_sign_col.split(\" \") # le colonne sono separate dallo spazio\n",
        "file.close() \n",
        "cat_sign_col.pop() # levo l'ultimo elemento che è vuoto"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpvugRX9X0iW"
      },
      "source": [
        "## 2.3 Analisi delle correlazioni\n",
        "\n",
        "#### Funzioni:\n",
        "\n",
        "* `dropColNotSign(dataset, col_sign, not_ignore = None)`: funzione per eliminare le colonne non significative dal dataset. `not_ignore`: colonna da ritenere significativa anche se non presente nella lista `col_sign`. \n",
        "\n",
        "* `corr_matrix_plot(dataset, corr_matrix)`: funzione per plottare la matrice di correlazione. \n",
        "\n",
        "* `highest_correlations(corr_matrix, tresh = 0.8)`: restituisce le features con correlazione superiore alla soglia `tresh` specificata.\n",
        "\n",
        "* `corr_dict(corr)`: restituisce un dizionario i cui elementi sono le coppie di features correlate, con relativo valore di correlazione. \n",
        "\n",
        "#### Variabili: \n",
        "\n",
        "`col_sign` è una lista che conterrà le colonne significative, cioè quelle selezionate per portare avanti l'analisi. \n",
        "\n",
        "`corr_d`: dizionario con le coppie di features correlate e relativa correlazione. \n",
        "\n",
        "#### Riferimenti \n",
        "\n",
        "* Riferimento sul sort: https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\n",
        "\n",
        "- Riferimento `drop_duplicates`: https://stackoverflow.com/questions/30530663/how-to-select-distinct-across-multiple-data-frame-columns-in-pandas\n",
        "\n",
        "* Riferimento `corr_sorted[corr_sorted>tresh]`: https://stackoverflow.com/questions/32067054/remove-rows-of-zeros-from-a-pandas-series\n",
        "\n",
        "* Riferimento `corr_dict`: https://stackoverflow.com/questions/25929319/how-to-iterate-over-pandas-multiindex-dataframe-using-index\n",
        "\n",
        "* Riferimento `np.isnan`: https://numpy.org/doc/stable/reference/generated/numpy.isnan.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "V40aPMq1X0iX"
      },
      "source": [
        "def dropColNotSign(dataset, col_sign, not_ignore = None):\n",
        "    if not fraud:\n",
        "        for col in dataset.columns:\n",
        "            if col not in col_sign and col != not_ignore:\n",
        "                dataset = dataset.drop([col], axis=1)\n",
        "    return dataset\n",
        "\n",
        "def corr_matrix_plot(dataset, corr_matrix):\n",
        "    f = plt.figure(figsize=(20, 20))\n",
        "    plt.matshow(corr_matrix, fignum=f.number)\n",
        "    plt.xticks(range(dataset.shape[1]), dataset.columns, fontsize=10, rotation=90)\n",
        "    plt.yticks(range(dataset.shape[1]), dataset.columns, fontsize=10)\n",
        "    cb = plt.colorbar()\n",
        "    cb.ax.tick_params(labelsize=14)\n",
        "    plt.title('Correlation Matrix', fontsize=16);\n",
        "    \n",
        "def highest_correlations(corr_matrix, tresh = 0.8):\n",
        "    corr_matrix_abs = corr_matrix.abs()\n",
        "    corr_matrix_abs = corr_matrix_abs.unstack()\n",
        "    corr_sorted = corr_matrix_abs.sort_values(kind=\"quicksort\")\n",
        "    corr_sorted = corr_sorted[:-len(num_sign_col)] # levo i termini sulla diagonale\n",
        "    corr_sorted = corr_sorted.drop_duplicates() # levo i termini doppi\n",
        "    corr_sorted = corr_sorted[corr_sorted>tresh]\n",
        "    return corr_sorted\n",
        "\n",
        "def corr_dict(corr):\n",
        "    corr_dict = {}\n",
        "    corr_list = []\n",
        "    for col1, df in corr.groupby(level=0):\n",
        "        for col2 in df:\n",
        "           if not np.isnan(corr.loc[col1,col2]):\n",
        "                corr_dict[col1,col2] = corr.loc[col1,col2]\n",
        "                corr_list.append([col1,col2])\n",
        "                corr_list.append([col2,col1])\n",
        "                \n",
        "    return corr_dict, corr_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UMbqDkl6X0iX"
      },
      "source": [
        "col_sign = num_sign_col + cat_sign_col # lista con le features significative\n",
        "print('Number of significative features: ', len(col_sign))\n",
        "correlation_data = dropColNotSign(mice_data, num_sign_col) # dataset solo con le features significative\n",
        "correlation_data_fraud = dropColNotSign(mice_data, num_sign_col, not_ignore = 'isFraud') \n",
        "correlation_data_fraud = correlation_data_fraud[correlation_data_fraud['isFraud'] == 1 ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rtjyiKqcX0iX"
      },
      "source": [
        "corr_matrix = correlation_data.corr()\n",
        "corr_matrix_plot(correlation_data, corr_matrix)\n",
        "highest_corr = highest_correlations(corr_matrix)\n",
        "highest_corr= highest_corr.unstack(level = 1)\n",
        "\n",
        "corr_matrix_fraud = correlation_data_fraud.corr()\n",
        "corr_matrix_plot(correlation_data_fraud, corr_matrix_fraud)\n",
        "highest_corr_fraud = highest_correlations(corr_matrix_fraud)\n",
        "highest_corr_fraud = highest_corr_fraud.unstack(level = 1)\n",
        "\n",
        "corr_d, corr_list = corr_dict(highest_corr)\n",
        "corr_d_fraud, corr_list_fraud = corr_dict(highest_corr_fraud)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "91djjeN5X0iX"
      },
      "source": [
        "for item in corr_list_fraud:\n",
        "    if item not in corr_list:\n",
        "        print(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1AyDKubX0iY"
      },
      "source": [
        "Riferimento `scatterplot`: https://seaborn.pydata.org/generated/seaborn.scatterplot.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RRhJ3C0CX0iY"
      },
      "source": [
        "sns.scatterplot(data=correlation_data[['C7','C12']], x=\"C7\", y=\"C12\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAqXbMxNX0iY"
      },
      "source": [
        "Fare differenza delle correlazioni nel caso fraud e safe per vedere se spuntano correlazioni diverse. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1-eM6qwvX0iY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpbwsvUFX0iY"
      },
      "source": [
        "# 3.1 Train validation split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-XUASHDX0iY"
      },
      "source": [
        "Splitto ogni set corrispondente ad un giorno in train e validation set. \n",
        "\n",
        "Il test set non è ancora stato caricato. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "t_nZgKFXX0iZ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train = {}\n",
        "X_val = {}\n",
        "y_train = {}\n",
        "y_val = {}\n",
        "\n",
        "for i in range(n_set):\n",
        "    X_train[i], X_val[i], y_train[i], y_val[i] = train_test_split(X[i], y[i], test_size=0.3, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6qJmZ_uX0iZ"
      },
      "source": [
        "## 3. Model selection\n",
        "\n",
        "Potrebbe essere utile andare a valutare le prestazioni del modello con e senza le feature ingegnerizzate Vxxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpRvec65X0iZ"
      },
      "source": [
        "Provo ad usare AdaBoost su un singolo subset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "87MYbKFjX0iZ"
      },
      "source": [
        "# https://scikit-learn.org/stable/modules/ensemble.html\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# for i in range(n_set):\n",
        "clf = AdaBoostClassifier(n_estimators=100)\n",
        "scores = cross_val_score(clf, X_train[0], y_train[0], cv=5)\n",
        "scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rUl1jDE3X0iZ"
      },
      "source": [
        "# valuto le performance del modello\n",
        "clf.fit(X_train[0], y_train[0])\n",
        "y_pred = clf.predict(X_val[0])\n",
        "cm = confusion_matrix(y_val[0], y_pred)\n",
        "cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opJiG9ubX0ia"
      },
      "source": [
        "Prova del balanced random forest di imblearn. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "id": "Lt1of4DfX0ia"
      },
      "source": [
        "# from sklearn.ensemble import AdaBoostClassifier\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "# from sklearn.metrics import balanced_accuracy_score\n",
        "# from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "# from imblearn.metrics import geometric_mean_score\n",
        "\n",
        "# brf = BalancedRandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
        "\n",
        "# brf.fit(X_train[i], y_train[i])\n",
        "# y_pred_brf = brf.predict(X_val[i])\n",
        "\n",
        "# print('Balanced Random Forest classifier performance:')\n",
        "# print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
        "#       .format(balanced_accuracy_score(y_val[i], y_pred_brf),\n",
        "#               geometric_mean_score(y_val[i], y_pred_brf)))\n",
        "# cm_brf = confusion_matrix(y_val[i], y_pred_brf)\n",
        "# cm_brf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaZRxgjQX0ia"
      },
      "source": [
        "Prova di EasyEnsemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Sx1MX6tGX0ia"
      },
      "source": [
        "# from imblearn.ensemble import EasyEnsembleClassifier\n",
        "# # Apply Easy Ensemble\n",
        "# ee = EasyEnsembleClassifier(n_estimators=5)\n",
        "# X_resampled, y_resampled = ee.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}