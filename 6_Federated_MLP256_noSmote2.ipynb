{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_Federated.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPkv6ltnruIvDfh+0fvM0mO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandrotofani/Tesi_magistrale/blob/master/6_Federated_MLP256_noSmote2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XuZ_oMl43ft"
      },
      "source": [
        "Overview: https://www.tensorflow.org/federated\r\n",
        "\r\n",
        "Image classification tutorial: https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOKJPHxH4j9j"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgkJJpzD4gVs"
      },
      "source": [
        "!pip install --quiet fastai==2.2.5\r\n",
        "!pip install --quiet folium==0.2.1\r\n",
        "!pip install --quiet imgaug==0.2.5\r\n",
        "!pip install --quiet tensorflow==2.3.0\r\n",
        "!pip install --quiet tensorflow_federated==0.17.0\r\n",
        "!pip install --quiet --upgrade nest_asyncio"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6Oguz1e4p29"
      },
      "source": [
        "import nest_asyncio\r\n",
        "nest_asyncio.apply()\r\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u_qzOek4tlY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c473b7b7-7c44-4399-f5e5-da2ae05b282e"
      },
      "source": [
        "import collections\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_federated as tff\r\n",
        "import pandas as pd \r\n",
        "import os\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5VvCBjWmrJC"
      },
      "source": [
        "import sys \n",
        "sys.path.append('/content/drive/MyDrive/Tesi_magistrale/Tesi_magistrale')\n",
        "import mf\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDXvW9iFxqft"
      },
      "source": [
        "# Import data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8nvRkt1yB7_"
      },
      "source": [
        "I dati vengono importati e poi splittati in train e test. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU2jrVgkla8Y"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Tesi_magistrale/Dataset/IEEE/Output/data.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67D33H0kldvF"
      },
      "source": [
        "data = mf.new_processing(data)\n",
        "data = pd.get_dummies(data)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yGwMV9Us42h"
      },
      "source": [
        "col_name = mf.get_col(data)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtEnrz70T013",
        "outputId": "b05bf08f-b2f8-4f87-e4d7-951edde825e4"
      },
      "source": [
        "print('Rate safe/fraud:', (1/mf.ratio(data)).round(3))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rate safe/fraud: 0.036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InAYt4jcL1-Y"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_data, test_data = train_test_split(data, test_size=0.2)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl58v1U4XyO5"
      },
      "source": [
        "Smote: https://imbalanced-learn.org/stable/generated/imblearn.over_sampling.SMOTE.html\n",
        "\n",
        "RandomUnderSampler: https://imbalanced-learn.org/stable/generated/imblearn.under_sampling.RandomUnderSampler.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3Jg0KuzJ6qq"
      },
      "source": [
        "def underSampling(data, frac_under=0.05):\n",
        "  from imblearn.under_sampling import RandomUnderSampler \n",
        "  us = RandomUnderSampler(sampling_strategy=frac_under, random_state=42)\n",
        "  y = data['isFraud']\n",
        "  X = data.drop(columns = ['isFraud'])\n",
        "  X_us, y_us = us.fit_resample(X, y)\n",
        "  return X_us, y_us\n",
        "\n",
        "def overSampling(X, y, frac_over=0.1):\n",
        "  from imblearn.over_sampling import SMOTE\n",
        "  sm = SMOTE(sampling_strategy=frac_over, random_state=42)\n",
        "  X_sm, y_sm = sm.fit_resample(X, y)  \n",
        "  return X_sm, y_sm\n",
        "\n",
        "def mergeResult(X, y, col_name):\n",
        "  y_res = np.ndarray(shape=(np.shape(y)[0],1), buffer = y)\n",
        "  data = np.concatenate((X,y_res), axis = 1)\n",
        "  col_name.append('isFraud')\n",
        "  dataset = pd.DataFrame(data=data, columns=col_name)  \n",
        "  return dataset\n",
        "\n",
        "def rate(y):\n",
        "  n_fraud = np.count_nonzero(y == 1)\n",
        "  n_safe = np.shape(y)[0] - n_fraud\n",
        "  return f'Rate safe/fraud: {n_safe/n_fraud}'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4cddCAqK4pY"
      },
      "source": [
        "# X, y = underSampling(train_data)\n",
        "# rate(y)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqiPC-MkK_BX"
      },
      "source": [
        "# X, y = overSampling(X, y)\n",
        "# rate(y)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHpsO2hJLFeR"
      },
      "source": [
        "# train_data = mergeResult(X, y, col_name)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bpOGbYXMAwY",
        "outputId": "853e3e0c-cbef-4f82-c920-ee6ef969dbdd"
      },
      "source": [
        "# print('Rate safe/fraud:', (mf.ratio(train_data)).round(3))\n",
        "print('Dataset size:', train_data.shape[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size: 472426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GawBnA83QyYj"
      },
      "source": [
        "del data#, X, y"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU9MLDmI2b_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d38cc273-4821-4bab-d914-368dcd2cd711"
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(472426, 734)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhZXxAzVxt5B"
      },
      "source": [
        "# Convert data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyqEukFeyHXA"
      },
      "source": [
        "Il dataset deve essere convertito in un tensore, con componenti (feature_vector, label). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuQfRXzrqoW2"
      },
      "source": [
        "def to_tensor(data, n_clients = 4):\n",
        "  shuffled = data.sample(frac=1)\n",
        "  result = np.array_split(shuffled, n_clients)  \n",
        "\n",
        "  res = []\n",
        "  label = []\n",
        "\n",
        "  for dataset in result:\n",
        "    label.append(dataset['isFraud'])\n",
        "    res.append(dataset.drop(columns = ['isFraud']))\n",
        "\n",
        "  dataset = {}\n",
        "  for i in range(n_clients):\n",
        "    dataset[i] = tf.data.Dataset.from_tensor_slices((res[i], label[i]))\n",
        "  return dataset"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LozyQwZOrCub"
      },
      "source": [
        "dataset = to_tensor(train_data)\n",
        "test_set = to_tensor(test_data)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0_iMvxDQs6l"
      },
      "source": [
        "del train_data, test_data"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbRUraJkxzmR"
      },
      "source": [
        "# Federated data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWq8QN4vyP1t"
      },
      "source": [
        "Si definisce la funzione di preprocessing del dataset, che serve a creare l'OrderedDict, su cui si andranno a creare le batch necessarie per il training del modello. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHxlJrXRs9qP"
      },
      "source": [
        "NUM_CLIENTS = 4\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 250\n",
        "SHUFFLE_BUFFER = 10\n",
        "PREFETCH_BUFFER = 10\n",
        "\n",
        "def preprocess(dataset):\n",
        "  def batch_format_fn(e1, e2):\n",
        "    return collections.OrderedDict(\n",
        "        x = tf.cast(e1, tf.float32),\n",
        "        y = tf.cast(e2, tf.int32))\n",
        "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(\n",
        "      BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)\n",
        "\n",
        "preprocessed_example_dataset = preprocess(dataset[0])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcIsH9tRyc0p"
      },
      "source": [
        "I dati federati sono una lista di dataset divisi per cliente. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6coH5NXYrQ8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "867b3f09-7879-4c24-9e01-7dcf413c076a"
      },
      "source": [
        "def make_federated_data(dataset):\n",
        "  federated = []\n",
        "  for i in dataset:\n",
        "    federated.append(preprocess(dataset[i]))\n",
        "  return federated\n",
        "\n",
        "federated_train_data = make_federated_data(dataset)\n",
        "\n",
        "print('Number of client datasets: {l}'.format(l=len(federated_train_data)))\n",
        "print('First dataset: {d}'.format(d=federated_train_data[0]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of client datasets: 4\n",
            "First dataset: <PrefetchDataset shapes: OrderedDict([(x, (None, 733)), (y, (None,))]), types: OrderedDict([(x, tf.float32), (y, tf.int32)])>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doNQKip2REvH"
      },
      "source": [
        "del dataset"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57xoboqEx25g"
      },
      "source": [
        "# Model creation and training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oACFRKebyl3u"
      },
      "source": [
        "Creazione della rete neurale che sarà trainata. \n",
        "\n",
        "Viene anche definita la model function, in cui si specifica il modello, il tipo di input, la loss e le metriche da utilizzare. \n",
        "\n",
        "Infine si costruisce il processo di averaging, specificando l'optimizer da usare, cioè SGD, e il learning rate del server e del client. \n",
        "\n",
        "BatchNormalization Layer: https://stackoverflow.com/questions/56514398/why-the-tensorflow-federated-performance-is-worse-than-single-keras-model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4A7FqBv8G7f"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, Input, BatchNormalization\n",
        "\n",
        "def create_keras_model():\n",
        "  model = Sequential()\n",
        "  model.add(Input(shape=(733,)))   \n",
        "  model.add(Dense(256, activation='relu')) \n",
        "  # model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.02))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  return model\n",
        "\n",
        "def model_fn():\n",
        "  soglia = 0.5\n",
        "  keras_model = create_keras_model()\n",
        "  return tff.learning.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec=preprocessed_example_dataset.element_spec,\n",
        "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.BinaryAccuracy(), \n",
        "               tf.keras.metrics.Recall(thresholds=soglia),\n",
        "               tf.keras.metrics.Precision(thresholds=soglia)])\n",
        "  \n",
        "iterative_process = tff.learning.build_federated_averaging_process(\n",
        "    model_fn,  \n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.05, momentum=0.6), #0.05\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1)) "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUUQi8kdzB3_"
      },
      "source": [
        "Training del modello. \n",
        "\n",
        "Gpu usage: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjLvHl-fA7wW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ace9bd-0354-4910-c861-4b73b14a5f6b"
      },
      "source": [
        "NUM_ROUNDS = 80\n",
        "state = iterative_process.initialize()\n",
        "for round_num in range(1, NUM_ROUNDS + 1):\n",
        "  state, metrics = iterative_process.next(state, federated_train_data)\n",
        "  print('round {:2d}, metrics={}'.format(round_num, metrics))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "round  1, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.977952), ('recall', 0.44731942), ('precision', 0.8467298), ('loss', 0.074027896)]))])\n",
            "round  2, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9779926), ('recall', 0.44867036), ('precision', 0.84678996), ('loss', 0.073857814)]))])\n",
            "round  3, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.97804624), ('recall', 0.4503925), ('precision', 0.8470148), ('loss', 0.07369722)]))])\n",
            "round  4, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.97811115), ('recall', 0.4517556), ('precision', 0.84814173), ('loss', 0.07348036)]))])\n",
            "round  5, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9781293), ('recall', 0.45342907), ('precision', 0.84685236), ('loss', 0.07326162)]))])\n",
            "round  6, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9781925), ('recall', 0.45473135), ('precision', 0.8479931), ('loss', 0.07301601)]))])\n",
            "round  7, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9782437), ('recall', 0.45630744), ('precision', 0.8482946), ('loss', 0.07282064)]))])\n",
            "round  8, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9782993), ('recall', 0.45857725), ('precision', 0.84787184), ('loss', 0.07271916)]))])\n",
            "round  9, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.97836536), ('recall', 0.45989168), ('precision', 0.84919375), ('loss', 0.07245811)]))])\n",
            "round 10, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.97843903), ('recall', 0.4619181), ('precision', 0.8498606), ('loss', 0.07230478)]))])\n",
            "round 11, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.97846466), ('recall', 0.4625814), ('precision', 0.8501957), ('loss', 0.07203707)]))])\n",
            "round 12, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.97852045), ('recall', 0.464407), ('precision', 0.8503744), ('loss', 0.071923345)]))])\n",
            "round 13, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9786045), ('recall', 0.46656728), ('precision', 0.8513324), ('loss', 0.071651556)]))])\n",
            "round 14, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9786242), ('recall', 0.46757135), ('precision', 0.850963), ('loss', 0.07153104)]))])\n",
            "round 15, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.97870815), ('recall', 0.46936044), ('precision', 0.8523876), ('loss', 0.071349755)]))])\n",
            "round 16, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9787614), ('recall', 0.47138077), ('precision', 0.8521672), ('loss', 0.07107292)]))])\n",
            "round 17, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9787787), ('recall', 0.47237265), ('precision', 0.85161823), ('loss', 0.070961796)]))])\n",
            "round 18, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9787747), ('recall', 0.47246394), ('precision', 0.8513718), ('loss', 0.07089416)]))])\n",
            "round 19, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9788715), ('recall', 0.474752), ('precision', 0.85277367), ('loss', 0.070607364)]))])\n",
            "round 20, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.97894657), ('recall', 0.47682104), ('precision', 0.8534582), ('loss', 0.07041848)]))])\n",
            "round 21, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9789664), ('recall', 0.47744173), ('precision', 0.8535001), ('loss', 0.07031429)]))])\n",
            "round 22, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9789736), ('recall', 0.4790726), ('precision', 0.85179174), ('loss', 0.07019042)]))])\n",
            "round 23, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.97903657), ('recall', 0.4806122), ('precision', 0.8525459), ('loss', 0.06999952)]))])\n",
            "round 24, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9790763), ('recall', 0.4814824), ('precision', 0.8532514), ('loss', 0.069828)]))])\n",
            "round 25, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.97915006), ('recall', 0.48330188), ('precision', 0.8541454), ('loss', 0.06968852)]))])\n",
            "round 26, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.97918), ('recall', 0.4842147), ('precision', 0.8543436), ('loss', 0.06959417)]))])\n",
            "round 27, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.97924024), ('recall', 0.48585165), ('precision', 0.85494614), ('loss', 0.06941422)]))])\n",
            "round 28, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.97927094), ('recall', 0.48739123), ('precision', 0.854298), ('loss', 0.069211625)]))])\n",
            "round 29, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9793353), ('recall', 0.4885596), ('precision', 0.85561585), ('loss', 0.06906858)]))])\n",
            "round 30, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.97940993), ('recall', 0.49074423), ('precision', 0.85608435), ('loss', 0.06887222)]))])\n",
            "round 31, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9793953), ('recall', 0.49040344), ('precision', 0.85596234), ('loss', 0.06876026)]))])\n",
            "round 32, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9794281), ('recall', 0.4913345), ('precision', 0.8561506), ('loss', 0.06865348)]))])\n",
            "round 33, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9794553), ('recall', 0.49335483), ('precision', 0.85485774), ('loss', 0.06845545)]))])\n",
            "round 34, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9795498), ('recall', 0.4943589), ('precision', 0.8575832), ('loss', 0.06832864)]))])\n",
            "round 35, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9795204), ('recall', 0.4949492), ('precision', 0.8556985), ('loss', 0.06822678)]))])\n",
            "round 36, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9795789), ('recall', 0.4965618), ('precision', 0.8561446), ('loss', 0.06804314)]))])\n",
            "round 37, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9796416), ('recall', 0.49732855), ('precision', 0.85788065), ('loss', 0.06787933)]))])\n",
            "round 38, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.97964066), ('recall', 0.49832654), ('precision', 0.856581), ('loss', 0.06774152)]))])\n",
            "round 39, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9797148), ('recall', 0.5006511), ('precision', 0.8568661), ('loss', 0.06764148)]))])\n",
            "round 40, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.97980833), ('recall', 0.502495), ('precision', 0.85859114), ('loss', 0.06746727)]))])\n",
            "round 41, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.97979486), ('recall', 0.50202644), ('precision', 0.85854036), ('loss', 0.06732478)]))])\n",
            "round 42, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9798574), ('recall', 0.50376076), ('precision', 0.8590835), ('loss', 0.06717036)]))])\n",
            "round 43, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.97984725), ('recall', 0.5043875), ('precision', 0.85787326), ('loss', 0.06712066)]))])\n",
            "round 44, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.97989625), ('recall', 0.50585407), ('precision', 0.8582269), ('loss', 0.066890314)]))])\n",
            "round 45, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.979895), ('recall', 0.5061766), ('precision', 0.85772914), ('loss', 0.06682754)]))])\n",
            "round 46, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.98001176), ('recall', 0.50848293), ('precision', 0.8597755), ('loss', 0.066599876)]))])\n",
            "round 47, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9800086), ('recall', 0.50913405), ('precision', 0.8588792), ('loss', 0.06649529)]))])\n",
            "round 48, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.97999215), ('recall', 0.5091462), ('precision', 0.8582566), ('loss', 0.06641912)]))])\n",
            "round 49, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9800754), ('recall', 0.51084405), ('precision', 0.8596371), ('loss', 0.066349)]))])\n",
            "round 50, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9801596), ('recall', 0.5124262), ('precision', 0.86117077), ('loss', 0.06612762)]))])\n",
            "round 51, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9801569), ('recall', 0.51273656), ('precision', 0.86064494), ('loss', 0.06605977)]))])\n",
            "round 52, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.98016804), ('recall', 0.51415443), ('precision', 0.8594782), ('loss', 0.06591113)]))])\n",
            "round 53, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9802443), ('recall', 0.51614434), ('precision', 0.86020565), ('loss', 0.06586997)]))])\n",
            "round 54, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.98023653), ('recall', 0.51551753), ('precision', 0.8606274), ('loss', 0.065729916)]))])\n",
            "round 55, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.98026013), ('recall', 0.5166251), ('precision', 0.86023045), ('loss', 0.06560272)]))])\n",
            "round 56, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9802948), ('recall', 0.51784825), ('precision', 0.86023617), ('loss', 0.06555937)]))])\n",
            "round 57, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9803489), ('recall', 0.5188158), ('precision', 0.86132973), ('loss', 0.06534937)]))])\n",
            "round 58, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.98035836), ('recall', 0.5197347), ('precision', 0.8606381), ('loss', 0.06528039)]))])\n",
            "round 59, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9804396), ('recall', 0.52107954), ('precision', 0.8623002), ('loss', 0.06513122)]))])\n",
            "round 60, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9804519), ('recall', 0.52156025), ('precision', 0.8622188), ('loss', 0.06504021)]))])\n",
            "round 61, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9804416), ('recall', 0.52248526), ('precision', 0.860726), ('loss', 0.06489084)]))])\n",
            "round 62, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9804888), ('recall', 0.52306336), ('precision', 0.8619635), ('loss', 0.06480343)]))])\n",
            "round 63, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.98047245), ('recall', 0.52382404), ('precision', 0.86043864), ('loss', 0.06474836)]))])\n",
            "round 64, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9805531), ('recall', 0.5248768), ('precision', 0.86240923), ('loss', 0.064660236)]))])\n",
            "round 65, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9805769), ('recall', 0.5254914), ('precision', 0.8625998), ('loss', 0.06456015)]))])\n",
            "round 66, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.98061466), ('recall', 0.52707964), ('precision', 0.86227834), ('loss', 0.0643441)]))])\n",
            "round 67, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.980649), ('recall', 0.52737784), ('precision', 0.8633592), ('loss', 0.064274386)]))])\n",
            "round 68, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9806879), ('recall', 0.52894175), ('precision', 0.8630735), ('loss', 0.06416042)]))])\n",
            "round 69, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.98075026), ('recall', 0.53007364), ('precision', 0.8642682), ('loss', 0.063982874)]))])\n",
            "round 70, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9807952), ('recall', 0.5314428), ('precision', 0.8644593), ('loss', 0.063914135)]))])\n",
            "round 71, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9807197), ('recall', 0.530244), ('precision', 0.86273986), ('loss', 0.06389254)]))])\n",
            "round 72, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9807737), ('recall', 0.5319722), ('precision', 0.8629373), ('loss', 0.06376419)]))])\n",
            "round 73, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.98080283), ('recall', 0.53289723), ('precision', 0.8630746), ('loss', 0.06370202)]))])\n",
            "round 74, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9808276), ('recall', 0.53317714), ('precision', 0.86366415), ('loss', 0.06362662)]))])\n",
            "round 75, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9808945), ('recall', 0.5347715), ('precision', 0.8645253), ('loss', 0.06345003)]))])\n",
            "round 76, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.98090494), ('recall', 0.53519136), ('precision', 0.8644048), ('loss', 0.063343816)]))])\n",
            "round 77, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9809251), ('recall', 0.5365058), ('precision', 0.8637094), ('loss', 0.063217565)]))])\n",
            "round 78, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.98095113), ('recall', 0.5368953), ('precision', 0.86425173), ('loss', 0.06317553)]))])\n",
            "round 79, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.98096395), ('recall', 0.53852004), ('precision', 0.8629365), ('loss', 0.06313918)]))])\n",
            "round 80, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_accuracy', 0.9810022), ('recall', 0.5381975), ('precision', 0.8647385), ('loss', 0.063005134)]))])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA4yp86ex9NK"
      },
      "source": [
        "# Model evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ_L_WaVzGX4"
      },
      "source": [
        "Evaluation del modello sui test data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-43k3etjd8P"
      },
      "source": [
        "evaluation = tff.learning.build_federated_evaluation(model_fn)\n",
        "train_metrics = evaluation(state.model, federated_train_data)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECb3PNHFjk_k"
      },
      "source": [
        "federated_test_data = make_federated_data(test_set)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoNPIG8Qj0G8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5178ce4b-819d-4794-f56b-3e64d5a30846"
      },
      "source": [
        "test_metrics = evaluation(state.model, federated_test_data)\n",
        "str(test_metrics)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"OrderedDict([('binary_accuracy', 0.9748878), ('recall', 0.40789786), ('precision', 0.78839123), ('loss', 0.091634825)])\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2TBCVOYzJal"
      },
      "source": [
        "Board di tensorboad, per visualizzare la loss e le metriche in modo interattivo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezubvjWqluK-"
      },
      "source": [
        "# logdir = \"/tmp/logs/scalars/training/\"\n",
        "# summary_writer = tf.summary.create_file_writer(logdir)\n",
        "# state = iterative_process.initialize()\n",
        "# with summary_writer.as_default():\n",
        "#   for round_num in range(1, NUM_ROUNDS):\n",
        "#     state, metrics = iterative_process.next(state, federated_train_data)\n",
        "#     for name, value in metrics['train'].items():\n",
        "#       tf.summary.scalar(name, value, step=round_num)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM1FgnQao2-h"
      },
      "source": [
        "# !ls {logdir}\n",
        "# %tensorboard --logdir {logdir} --port=0"
      ],
      "execution_count": 42,
      "outputs": []
    }
  ]
}